{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn.utils.extmath import softmax\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (11834, 784)\n",
      "X_test.shape (2946, 784)\n",
      "y_train.shape (11834, 2)\n",
      "y_test.shape (2946, 2)\n",
      "y_test [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "def sample_multiclass_MNIST(list_digits=['0','1', '2'], full_MNIST=None):\n",
    "    # get train and test set from MNIST of given digits\n",
    "    # e.g., list_digits = ['0', '1', '2']\n",
    "    if full_MNIST is not None:\n",
    "        X, y = full_MNIST\n",
    "    else:\n",
    "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "        X = X / 255.\n",
    "    Y = list2onehot(y.tolist(), list_digits)\n",
    "    \n",
    "    idx = [i for i in np.arange(len(y)) if y[i] in list_digits] # list of indices where the label y is in list_digits\n",
    "    \n",
    "    X01 = X[idx,:]\n",
    "    y01 = Y[idx,:]\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_test = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "    y_train = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "\n",
    "    for i in np.arange(X01.shape[0]):\n",
    "        # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "        U = np.random.rand() # Uniform([0,1]) variable\n",
    "        if U<0.8:\n",
    "            X_train.append(X01[i,:])\n",
    "            y_train.append(y01[i,:].copy())\n",
    "        else:\n",
    "            X_test.append(X01[i,:])\n",
    "            y_test.append(y01[i,:].copy())\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_train = np.asarray(y_train)\n",
    "    y_test = np.asarray(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# test \n",
    "X_train, X_test, y_train, y_test = sample_multiclass_MNIST(list_digits=['0','1'], full_MNIST=None)\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "print('y_test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some common activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid and logit function \n",
    "def sigmoid(x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(2*x)-1)/(np.exp(2*x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sigmoid function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[15,2.5])\n",
    "x = np.linspace(-7, 7, 100)\n",
    "ax.plot(x, sigmoid(x), color='blue', label=\"sigmoid\")\n",
    "ax.plot(x, ReLU(x), color='red', label=\"ReLU\")\n",
    "ax.plot(x, tanh(x), color='gray', label=\"tanh\")\n",
    "ax.set_ylim(-1,2)\n",
    "\n",
    "plt.axhline(y=1, color='g', linestyle='--')\n",
    "plt.axvline(x=0, color='g', linestyle='--')\n",
    "ax.legend(fontsize=15)\n",
    "plt.savefig('activation.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Class FFNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN():\n",
    "\n",
    "    def __init__(self,\n",
    "                 list_hidden_layer_sizes = [30], # hidden1, hidden2, .. , hidden h\n",
    "                 loss_function = 'softmax-cross-entropy', # or 'square' or 'cross-entropy'\n",
    "                 activation_list = None, # ['ReLU', 'sigmoid'],\n",
    "                 node_states = None,\n",
    "                 weight_matrices = None,\n",
    "                 training_set = [None, None]): # input = [feature_dim x samples], output [\\kappa x samples]\n",
    "                 \n",
    "        self.training_set = training_set\n",
    "        # self.test_set = test_set\n",
    "        self.list_layer_sizes = [self.training_set[0].shape[0]] + list_hidden_layer_sizes + [self.training_set[1].shape[0]]\n",
    "        self.loss_function = loss_function\n",
    "        self.n_layers = len(self.list_layer_sizes)-1\n",
    "        self.activation_list = activation_list\n",
    "        self.node_states = node_states\n",
    "        self.weight_matrices = weight_matrices\n",
    "        \n",
    "        self.initialize()\n",
    "        \n",
    "       \n",
    "    def initialize(self):\n",
    "        if self.activation_list is None:\n",
    "            activation_list = ['ReLU' for i in np.arange(len(self.list_layer_sizes)-1)]\n",
    "            activation_list[-1] = 'sigmoid'\n",
    "            self.activation_list = activation_list\n",
    "\n",
    "        if self.node_states == None:\n",
    "            node_states = []\n",
    "            for i in np.arange(len(self.list_layer_sizes)):\n",
    "                node_states.append(np.zeros(shape=[self.list_layer_sizes[i], ]))\n",
    "            self.node_states = node_states\n",
    "        \n",
    "        if self.weight_matrices == None:\n",
    "            weight_matrices = []\n",
    "            for i in np.arange(len(self.activation_list)):\n",
    "                U = np.random.rand(self.list_layer_sizes[i], self.list_layer_sizes[i+1]) \n",
    "                weight_matrices.append(1-2*U)\n",
    "            self.weight_matrices = weight_matrices\n",
    "\n",
    "       \n",
    "    def forward_propagate(self, input_data):\n",
    "        # Forward propagate the input using the current weights and update node states \n",
    "        self.node_states[0] = input_data\n",
    "        for i in np.arange(self.n_layers):    \n",
    "            X_new = self.node_states[i].T @ self.weight_matrices[i]\n",
    "            X_new = activation(X_new, type=self.activation_list[i])\n",
    "            self.node_states[i+1] = X_new\n",
    "            # print('!!! X_new', X_new)\n",
    "        \n",
    "    def backpropagate(self, output_data):\n",
    "        # Backpropagate the error and return the gradient of the weight matrices\n",
    "        # output_data = column array\n",
    "        node_errors = self.node_states.copy()\n",
    "\n",
    "        y = output_data\n",
    "        y_hat = self.node_states[-1] # shape (\\kappa, )\n",
    "        y_hat = y_hat[:, np.newaxis]\n",
    "        node_errors[-1] = delta_loss_function(y=y, y_hat=y_hat, type=self.loss_function)\n",
    "        W_grad = self.weight_matrices.copy()\n",
    "        \n",
    "        for i in range(self.n_layers -1, -1, -1):\n",
    "            # First weight the errors of nodes in layer above by the derivative of activation\n",
    "            wtd_errors = node_errors[i+1].copy() \n",
    "            z = self.node_states[i][:,np.newaxis]\n",
    "            W = self.weight_matrices[i]\n",
    "            layer_size_above = self.list_layer_sizes[i+1]\n",
    "            delta_activation_weights = [delta_activation(z.T @ W[:,q], type=self.activation_list[i]) for q in np.arange(layer_size_above)]\n",
    "            delta_activation_weights = np.asarray(delta_activation_weights)\n",
    "            if len(delta_activation_weights.shape)==1:\n",
    "                delta_activation_weights = delta_activation_weights[:,np.newaxis] \n",
    "            if len(wtd_errors.shape)==1:\n",
    "                wtd_errors = wtd_errors[:,np.newaxis] \n",
    "        \n",
    "            wtd_errors = wtd_errors * delta_activation_weights\n",
    "            # wtd_errors = wtd_errors[:, np.newaxis]\n",
    "\n",
    "            # Compute the gradient of the i th weight matrix (conneting layer i and i+1)\n",
    "            W_grad[i] = wtd_errors @ z.T  \n",
    "            \n",
    "            # Propagate it backward onto layer i\n",
    "            node_errors[i] = (W @ wtd_errors)[:,0]\n",
    "        return W_grad\n",
    "    \n",
    "    def minibatch_grad(self, minibatch_idx):\n",
    "        \n",
    "        W_grad_list = []\n",
    "        Y = self.training_set[1] # true labels: each column = one-hot encoding\n",
    "        X = self.training_set[0]\n",
    "        for i in minibatch_idx:\n",
    "            self.forward_propagate(input_data=X[:,i])\n",
    "            y = Y[:, i]\n",
    "            y = y[:, np.newaxis]\n",
    "            W_grad = self.backpropagate(output_data=y)\n",
    "            W_grad_list.append(W_grad)\n",
    "            \n",
    "        W_grad_minibatch = self.weight_matrices.copy()\n",
    "        for j in np.arange(self.n_layers):\n",
    "            grad_temp = [W_grad_list[i][j] for i in np.arange(len(minibatch_idx))]\n",
    "            W_grad_minibatch[j] = np.sum(np.asarray(grad_temp), axis=0)\n",
    "            \n",
    "        return W_grad_minibatch\n",
    "        \n",
    "    def train(self, n_SGD_iter=10, minibatch_size=1, stopping_diff=0.01):\n",
    "        Y = self.training_set[1]\n",
    "        for i in np.arange(n_SGD_iter):\n",
    "            # compute the minibatch gradients of weight matrices \n",
    "            num_train_data = Y.shape[0]\n",
    "            minibatch_idx = np.random.choice(np.arange(num_train_data), minibatch_size)\n",
    "            W_grad_minibatch = self.minibatch_grad(minibatch_idx=minibatch_idx)\n",
    "            \n",
    "            # GD \n",
    "            for j in np.arange(self.n_layers):\n",
    "                W1 = self.weight_matrices[j]\n",
    "                t = 0\n",
    "                grad = W_grad_minibatch[j].T\n",
    "                if (np.linalg.norm(grad) > stopping_diff):\n",
    "                    W1 = W1 - (np.log(i+1) / (((i + 1) ** (0.5)))) * grad\n",
    "                self.weight_matrices[j] = W1.copy()\n",
    "                if j == 0:\n",
    "                    print('SGD epoch = %i, grad_norm = %f' %(i, np.linalg.norm(grad)))\n",
    "        \n",
    "    def predict(self, test_set, normalize=False):\n",
    "        y_pred = []\n",
    "        for i in np.arange(test_set.shape[1]):\n",
    "            self.forward_propagate(input_data=test_set[:,i])\n",
    "            y_hat = self.node_states[-1].copy()\n",
    "            if normalize:\n",
    "                y_hat /= np.sum(y_hat)\n",
    "            y_pred.append(y_hat)\n",
    "            # print('!! y_hat', y_hat)\n",
    "        return y_pred\n",
    "            \n",
    "        \n",
    "### Helper functions\n",
    "\n",
    "def loss_function(y, y_hat, type='cross-entropy'):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    if type == 'cross_entropy':\n",
    "        return cross-entropy(y=y, y_hat=y_hat)\n",
    "    elif type == 'square':\n",
    "        return (1/2) * (y_hat - y).T @ (y_hat - y)\n",
    "   \n",
    "\n",
    "def delta_loss_function(y, y_hat, type='cross-entropy'):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    # return delta_cross_entropy(y=y, y_hat=y_hat/np.sum(y_hat))\n",
    "    \n",
    "    if type == 'cross-entropy':\n",
    "        return delta_cross_entropy(y=y, y_hat=y_hat)\n",
    "    elif type == 'square':\n",
    "        return y_hat - y\n",
    "    elif type == 'softmax-cross-entropy':\n",
    "        return softmax(y_hat) - y\n",
    "\n",
    "        \n",
    "def activation(x, type='sigmoid'):\n",
    "    if type == 'sigmoid':\n",
    "        return 1/(1+np.exp(-x))\n",
    "    elif type == 'ReLU':\n",
    "        return np.maximum(0,x)\n",
    "    elif type == 'tanh':\n",
    "        return (np.exp(2*x)-1)/(np.exp(2*x)+1)\n",
    "\n",
    "def delta_activation(x, type='sigmoid'):\n",
    "    # derivate of activation function\n",
    "    if type == 'sigmoid':\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    elif type == 'ReLU':\n",
    "        return int((x>0))\n",
    "    elif type == 'tanh':\n",
    "        return (2/(np.exp(x)+np.exp(-x)))**2\n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(2*x)-1)/(np.exp(2*x)+1)\n",
    "\n",
    "def ssoftmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    z = (y.T @ np.log(y_hat))[0][0]\n",
    "    return (y.T @ np.log(y_hat))[0][0]\n",
    "\n",
    "def delta_cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    y_hat /= np.max(y_hat)\n",
    "    z = y.copy()\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        a = y.argmax(axis=0)[0]\n",
    "        z[i,0] = -1/y_hat[a, 0]\n",
    "    return z\n",
    "\n",
    "def list2onehot(y, list_classes):\n",
    "    \"\"\"\n",
    "    y = list of class lables of length n\n",
    "    output = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    \"\"\"\n",
    "    Y = np.zeros(shape = [len(y), len(list_classes)], dtype=int)\n",
    "    for i in np.arange(Y.shape[0]):\n",
    "        for j in np.arange(len(list_classes)):\n",
    "            if y[i] == list_classes[j]:\n",
    "                Y[i,j] = 1\n",
    "    return Y\n",
    "\n",
    "def onehot2list(y, list_classes=None):\n",
    "    \"\"\"\n",
    "    y = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    output =  list of class lables of length n\n",
    "    \"\"\"\n",
    "    if list_classes is None:\n",
    "        list_classes = np.arange(y.shape[1])\n",
    "        \n",
    "    y_list = []\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        idx = np.where(y[i,:]==1)\n",
    "        idx = idx[0][0]\n",
    "        y_list.append(list_classes[idx])\n",
    "    return y_list\n",
    "\n",
    "def softmax(a):\n",
    "    \"\"\"\n",
    "    given an array a = [a_1, .. a_k], compute the softmax distribution p = [p_1, .. , p_k] where p_i \\propto exp(a_i)\n",
    "    \"\"\"\n",
    "    a1 = a - np.max(a)\n",
    "    p = np.exp(a1)\n",
    "    if type(a) is list:\n",
    "        p = p/np.sum(p)\n",
    "    else: \n",
    "        row_sum = np.sum(p, axis=1)\n",
    "        p = p/row_sum[:, np.newaxis]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN = FFNN(list_hidden_layer_sizes = [30], # hidden1, hidden2, .. , hidden h\n",
    "                 loss_function = 'softmax-cross-entropy', # 'cross-entropy' or 'square'\n",
    "                 activation_list = None, # ['ReLU', 'sigmoid'],\n",
    "                 node_states = None,\n",
    "                 weight_matrices = None,\n",
    "                 training_set = [X_train.T/500, y_train.T]) # input = [N x ], output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 30, 2]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFNN.list_layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD epoch = 0, grad_norm = 6.181812\n",
      "SGD epoch = 1, grad_norm = 6.180917\n",
      "SGD epoch = 2, grad_norm = 10.025158\n",
      "SGD epoch = 3, grad_norm = 17.859440\n",
      "SGD epoch = 4, grad_norm = 5.357916\n",
      "SGD epoch = 5, grad_norm = 1.190363\n",
      "SGD epoch = 6, grad_norm = 0.777284\n",
      "SGD epoch = 7, grad_norm = 0.585844\n",
      "SGD epoch = 8, grad_norm = 0.459417\n",
      "SGD epoch = 9, grad_norm = 0.389665\n",
      "SGD epoch = 10, grad_norm = 0.340057\n",
      "SGD epoch = 11, grad_norm = 0.303101\n",
      "SGD epoch = 12, grad_norm = 0.252435\n",
      "SGD epoch = 13, grad_norm = 0.235867\n",
      "SGD epoch = 14, grad_norm = 0.214375\n",
      "SGD epoch = 15, grad_norm = 0.205210\n",
      "SGD epoch = 16, grad_norm = 0.186954\n",
      "SGD epoch = 17, grad_norm = 0.179365\n",
      "SGD epoch = 18, grad_norm = 0.168213\n",
      "SGD epoch = 19, grad_norm = 0.151192\n",
      "SGD epoch = 20, grad_norm = 0.148244\n",
      "SGD epoch = 21, grad_norm = 0.137716\n",
      "SGD epoch = 22, grad_norm = 0.132654\n",
      "SGD epoch = 23, grad_norm = 0.130307\n",
      "SGD epoch = 24, grad_norm = 0.122705\n",
      "SGD epoch = 25, grad_norm = 0.118524\n",
      "SGD epoch = 26, grad_norm = 0.114783\n",
      "SGD epoch = 27, grad_norm = 0.113597\n",
      "SGD epoch = 28, grad_norm = 0.102051\n",
      "SGD epoch = 29, grad_norm = 0.101635\n",
      "SGD epoch = 30, grad_norm = 0.098715\n",
      "SGD epoch = 31, grad_norm = 0.097226\n",
      "SGD epoch = 32, grad_norm = 0.093460\n",
      "SGD epoch = 33, grad_norm = 0.087434\n",
      "SGD epoch = 34, grad_norm = 0.088770\n",
      "SGD epoch = 35, grad_norm = 0.083563\n",
      "SGD epoch = 36, grad_norm = 0.084934\n",
      "SGD epoch = 37, grad_norm = 0.078756\n",
      "SGD epoch = 38, grad_norm = 0.078283\n",
      "SGD epoch = 39, grad_norm = 0.078067\n"
     ]
    }
   ],
   "source": [
    "FFNN.train(n_SGD_iter=40, minibatch_size=1000, stopping_diff=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = FFNN.predict(X_test.T/200, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(Y_test, P_pred, use_opt_threshold=False, verbose=True):\n",
    "    \n",
    "    # y_test = binary label \n",
    "    # P_pred = predicted probability for y_test\n",
    "    # compuate various binary classification accuracy metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, P_pred, pos_label=None)\n",
    "    mythre = thresholds[np.argmax(tpr - fpr)]\n",
    "    myauc = metrics.auc(fpr, tpr)\n",
    "    # print('!!! auc', myauc)\n",
    "    \n",
    "    # Compute classification statistics\n",
    "    threshold = 0.5\n",
    "    if use_opt_threshold:\n",
    "        threshold = mythre\n",
    "    \n",
    "    Y_pred = P_pred.copy()\n",
    "    Y_pred[Y_pred < threshold] = 0\n",
    "    Y_pred[Y_pred >= threshold] = 1\n",
    "\n",
    "    mcm = confusion_matrix(Y_test, Y_pred)\n",
    "    \n",
    "    tn = mcm[0, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    fp = mcm[0, 1]\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tn / (tn + fp)\n",
    "    specificity = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    fall_out = fp / (fp + tn)\n",
    "    miss_rate = fn / (fn + tp)\n",
    "\n",
    "    # Save results\n",
    "    results_dict = {}\n",
    "    results_dict.update({'Y_test': Y_test})\n",
    "    results_dict.update({'Y_pred': Y_pred})\n",
    "    results_dict.update({'AUC': myauc})\n",
    "    results_dict.update({'Opt_threshold': mythre})\n",
    "    results_dict.update({'Accuracy': accuracy})\n",
    "    results_dict.update({'Sensitivity': sensitivity})\n",
    "    results_dict.update({'Specificity': specificity})\n",
    "    results_dict.update({'Precision': precision})\n",
    "    results_dict.update({'Fall_out': fall_out})\n",
    "    results_dict.update({'Miss_rate': miss_rate})\n",
    "    results_dict.update({'Confusion_mx': mcm})\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        for key in [key for key in results_dict.keys()]:\n",
    "            if key not in ['Y_test', 'Y_pred', 'Confusion_mx']:\n",
    "                print('% s ===> %.3f' % (key, results_dict.get(key)))\n",
    "        print('Confusion matrix \\n ===>', mcm)\n",
    "            \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ===> 0.991\n",
      "Opt_threshold ===> 0.614\n",
      "Accuracy ===> 0.970\n",
      "Sensitivity ===> 0.943\n",
      "Specificity ===> 0.994\n",
      "Precision ===> 0.952\n",
      "Fall_out ===> 0.057\n",
      "Miss_rate ===> 0.006\n",
      "Confusion matrix \n",
      " ===> [[1299   79]\n",
      " [   9 1559]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Y_test': array([1, 0, 1, ..., 0, 0, 1]),\n",
       " 'Y_pred': array([1., 0., 1., ..., 0., 0., 1.]),\n",
       " 'AUC': 0.9909066674565327,\n",
       " 'Opt_threshold': 0.6142123610633841,\n",
       " 'Accuracy': 0.9701289884589274,\n",
       " 'Sensitivity': 0.9426705370101597,\n",
       " 'Specificity': 0.9942602040816326,\n",
       " 'Precision': 0.9517704517704517,\n",
       " 'Fall_out': 0.057329462989840346,\n",
       " 'Miss_rate': 0.005739795918367347,\n",
       " 'Confusion_mx': array([[1299,   79],\n",
       "        [   9, 1559]])}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute binary classification metrics on test data\n",
    "\n",
    "y_test_label = np.asarray(onehot2list(y_test))\n",
    "P_pred=np.asarray([p[1] for p in y_pred])\n",
    "\n",
    "compute_accuracy_metrics(Y_test=y_test_label, P_pred=P_pred, use_opt_threshold=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colourgraphenv",
   "language": "python",
   "name": "colourgraphenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
