{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn.utils.extmath import softmax\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from tqdm import trange\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some common activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid and logit function \n",
    "def sigmoid(x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sigmoid function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[15,2.5])\n",
    "x = np.linspace(-7, 7, 100)\n",
    "ax.plot(x, sigmoid(x), color='blue', label=\"sigmoid\")\n",
    "ax.plot(x, ReLU(x), color='red', label=\"ReLU\")\n",
    "ax.plot(x, tanh(x), color='gray', label=\"tanh\")\n",
    "ax.set_ylim(-1,2)\n",
    "\n",
    "plt.axhline(y=1, color='g', linestyle='--')\n",
    "plt.axvline(x=0, color='g', linestyle='--')\n",
    "ax.legend(fontsize=15)\n",
    "plt.savefig('activation.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Class FFNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying MNIST images using FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2onehot(y, list_classes):\n",
    "    \"\"\"\n",
    "    y = list of class lables of length n\n",
    "    output = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    \"\"\"\n",
    "    Y = np.zeros(shape = [len(y), len(list_classes)], dtype=int)\n",
    "    for i in np.arange(Y.shape[0]):\n",
    "        for j in np.arange(len(list_classes)):\n",
    "            if y[i] == list_classes[j]:\n",
    "                Y[i,j] = 1\n",
    "    return Y\n",
    "\n",
    "def onehot2list(y, list_classes=None):\n",
    "    \"\"\"\n",
    "    y = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    output =  list of class lables of length n\n",
    "    \"\"\"\n",
    "    if list_classes is None:\n",
    "        list_classes = np.arange(y.shape[1])\n",
    "        \n",
    "    y_list = []\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        idx = np.where(y[i,:]==1)\n",
    "        idx = idx[0][0]\n",
    "        y_list.append(list_classes[idx])\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (70000, 784)\n",
      "y.shape (70000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEach row of X is a vectroization of an image of 28 x 28 = 784 pixels.  \\nThe corresponding row of y holds the true class label from {0,1, .. , 9}.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "# X = X.values  ### Uncomment this line if you are having type errors in plotting. It is loading as a pandas dataframe, but our indexing is for numpy array. \n",
    "X = X / 255.\n",
    "\n",
    "print('X.shape', X.shape)\n",
    "print('y.shape', y.shape)\n",
    "\n",
    "'''\n",
    "Each row of X is a vectroization of an image of 28 x 28 = 784 pixels.  \n",
    "The corresponding row of y holds the true class label from {0,1, .. , 9}.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_multiclass_MNIST(list_digits=['0','1', '2'], full_MNIST=None):\n",
    "    # get train and test set from MNIST of given digits\n",
    "    # e.g., list_digits = ['0', '1', '2']\n",
    "    if full_MNIST is not None:\n",
    "        X, y = full_MNIST\n",
    "    else:\n",
    "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "        X = X / 255.\n",
    "    Y = list2onehot(y.tolist(), list_digits)\n",
    "    \n",
    "    idx = [i for i in np.arange(len(y)) if y[i] in list_digits] # list of indices where the label y is in list_digits\n",
    "    \n",
    "    X01 = X[idx,:]\n",
    "    y01 = Y[idx,:]\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_test = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "    y_train = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "\n",
    "    for i in trange(X01.shape[0]):\n",
    "        # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "        U = np.random.rand() # Uniform([0,1]) variable\n",
    "        if U<0.8:\n",
    "            X_train.append(X01[i,:])\n",
    "            y_train.append(y01[i,:].copy())\n",
    "        else:\n",
    "            X_test.append(X01[i,:])\n",
    "            y_test.append(y01[i,:].copy())\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_train = np.asarray(y_train)\n",
    "    y_test = np.asarray(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(Y_test, P_pred, use_opt_threshold=False, verbose=True):\n",
    "    \n",
    "    # y_test = binary label \n",
    "    # P_pred = predicted probability for y_test\n",
    "    # compuate various binary classification accuracy metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, P_pred, pos_label=None)\n",
    "    mythre = thresholds[np.argmax(tpr - fpr)]\n",
    "    myauc = metrics.auc(fpr, tpr)\n",
    "    # print('!!! auc', myauc)\n",
    "    \n",
    "    # Compute classification statistics\n",
    "    threshold = 0.5\n",
    "    if use_opt_threshold:\n",
    "        threshold = mythre\n",
    "    \n",
    "    Y_pred = P_pred.copy()\n",
    "    Y_pred[Y_pred < threshold] = 0\n",
    "    Y_pred[Y_pred >= threshold] = 1\n",
    "\n",
    "    mcm = confusion_matrix(Y_test, Y_pred)\n",
    "    \n",
    "    tn = mcm[0, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    fp = mcm[0, 1]\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tn / (tn + fp)\n",
    "    specificity = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    fall_out = fp / (fp + tn)\n",
    "    miss_rate = fn / (fn + tp)\n",
    "\n",
    "    # Save results\n",
    "    results_dict = {}\n",
    "    results_dict.update({'Y_test': Y_test})\n",
    "    results_dict.update({'Y_pred': Y_pred})\n",
    "    results_dict.update({'AUC': myauc})\n",
    "    results_dict.update({'Opt_threshold': mythre})\n",
    "    results_dict.update({'Accuracy': accuracy})\n",
    "    results_dict.update({'Sensitivity': sensitivity})\n",
    "    results_dict.update({'Specificity': specificity})\n",
    "    results_dict.update({'Precision': precision})\n",
    "    results_dict.update({'Fall_out': fall_out})\n",
    "    results_dict.update({'Miss_rate': miss_rate})\n",
    "    results_dict.update({'Confusion_mx': mcm})\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        for key in [key for key in results_dict.keys()]:\n",
    "            if key not in ['Y_test', 'Y_pred', 'Confusion_mx']:\n",
    "                print('% s ===> %.3f' % (key, results_dict.get(key)))\n",
    "        print('Confusion matrix  ===> \\n', mcm)\n",
    "            \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14780/14780 [00:00<00:00, 569950.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (784, 100)\n",
      "Y_train.shape (2, 100)\n",
      "X_ext.shape (785, 100)\n",
      "W0.shape (785, 10)\n",
      "W1.shape (11, 2)\n",
      "SGD epoch = 0, train_loss=43.798532\n",
      "SGD epoch = 1, train_loss=31.963892\n",
      "SGD epoch = 2, train_loss=33.470748\n",
      "SGD epoch = 3, train_loss=33.987928\n",
      "SGD epoch = 4, train_loss=25.206470\n",
      "SGD epoch = 5, train_loss=25.912275\n",
      "SGD epoch = 6, train_loss=25.997790\n",
      "SGD epoch = 7, train_loss=26.443237\n",
      "SGD epoch = 8, train_loss=25.747326\n",
      "SGD epoch = 9, train_loss=25.548254\n",
      "SGD epoch = 10, train_loss=25.215434\n",
      "SGD epoch = 11, train_loss=24.870648\n",
      "SGD epoch = 12, train_loss=24.636405\n",
      "SGD epoch = 13, train_loss=24.397612\n",
      "SGD epoch = 14, train_loss=24.254782\n",
      "SGD epoch = 15, train_loss=24.120878\n",
      "SGD epoch = 16, train_loss=24.038177\n",
      "SGD epoch = 17, train_loss=23.960726\n",
      "SGD epoch = 18, train_loss=23.904813\n",
      "SGD epoch = 19, train_loss=23.847433\n",
      "AUC ===> 0.992\n",
      "Opt_threshold ===> 0.404\n",
      "Accuracy ===> 0.479\n",
      "Sensitivity ===> 1.000\n",
      "Specificity ===> 0.000\n",
      "Precision ===> nan\n",
      "Fall_out ===> 0.000\n",
      "Miss_rate ===> 1.000\n",
      "Confusion matrix  ===> \n",
      " [[1393    0]\n",
      " [1516    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-846-3b653028e318>:30: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (tp + fp)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = sample_multiclass_MNIST(list_digits=['0','1'], full_MNIST=[X,y])\n",
    "\n",
    "# train_size = X_train.shape[0]\n",
    "\n",
    "train_size = 100\n",
    "\n",
    "idx = np.random.choice(np.arange(len(y_train)), train_size)\n",
    "X_train0 = X_train[idx, :]\n",
    "y_train0 = y_train[idx]\n",
    "\n",
    "\"\"\"\n",
    "NN = FFNN(list_hidden_layer_sizes = [], # hidden1, hidden2, .. , hidden h\n",
    "                     loss_function = 'softmax-cross-entropy', #'softmax-cross-entropy', # 'cross-entropy' or 'square'\n",
    "                     activation_list = None, # ['ReLU', 'sigmoid'],\n",
    "                     node_states = None,\n",
    "                     weight_matrices = None,\n",
    "                     training_set = [X_train0.T/400, y_train0.T]) # input = [N x ], output \n",
    "print('NN.list_layer_sizes', NN.list_layer_sizes)\n",
    "print('NN.training_set[0]', NN.training_set[0].shape)\n",
    "\"\"\"\n",
    "NN = MLP(M = 10, training_set = [X_train0.T/400, y_train0.T])\n",
    "\n",
    "NN.train(n_iter=20)\n",
    "\n",
    "y_pred_test = NN.predict(X_test.T/100, normalize=True)\n",
    "\n",
    "y_test_label = np.asarray(onehot2list(y_test))\n",
    "\n",
    "P_pred_test = y_pred_test[1,:]\n",
    "\n",
    "results_dict_test = compute_accuracy_metrics(Y_test=y_test_label, P_pred=P_pred_test, use_opt_threshold=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute binary classification metrics on test data\n",
    "\n",
    "M_list = [1, 5, 10, 20]\n",
    "accuracy_list_test = []\n",
    "accuracy_list_train = []\n",
    "\n",
    "for M in M_list:\n",
    "    NN = FFNN(list_hidden_layer_sizes = [M], # hidden1, hidden2, .. , hidden h\n",
    "                     loss_function = 'softmax-cross-entropy', # 'cross-entropy' or 'square'\n",
    "                     activation_list = None, # ['ReLU', 'sigmoid'],\n",
    "                     node_states = None,\n",
    "                     weight_matrices = None,\n",
    "                     training_set = [X_train.T/100, y_train.T]) # input = [N x ], output \n",
    "\n",
    "    NN.train(n_SGD_iter=40, minibatch_size=100, stopping_diff=0.01, verbose=False, L2_reg = 0)\n",
    "    y_pred_train = NN.predict(X_train.T/100, normalize=True)\n",
    "    y_pred_test = NN.predict(X_test.T/100, normalize=True)\n",
    "    \n",
    "    y_train_label = np.asarray(onehot2list(y_train))\n",
    "    y_test_label = np.asarray(onehot2list(y_test))\n",
    "\n",
    "    P_pred_train = np.asarray([p[1] for p in y_pred_train])\n",
    "    P_pred_test = np.asarray([p[1] for p in y_pred_test])\n",
    "\n",
    "    results_dict_train = compute_accuracy_metrics(Y_test=y_train_label, P_pred=P_pred_train, use_opt_threshold=False, verbose=False)\n",
    "    results_dict_test = compute_accuracy_metrics(Y_test=y_test_label, P_pred=P_pred_test, use_opt_threshold=False, verbose=False)\n",
    "    accuracy_list_test.append(results_dict_train.get('Accuracy'))\n",
    "    accuracy_list_train.append(results_dict_test.get('Accuracy'))\n",
    "    \n",
    "# make plot\n",
    "ncols = 1\n",
    "fig, ax = plt.subplots(nrows=1, ncols=ncols, figsize=[13,4])\n",
    "for i in np.arange(ncols):\n",
    "    ax.plot(M_list, accuracy_list_train, color='blue', label=\"train error\")\n",
    "    ax.plot(M_list, accuracy_list_test, color='red', label=\"test error\")\n",
    "    ax.set_xlabel('degree', fontsize=10)\n",
    "    # ax.title.set_text(\"num training ex = %i, \\n L2 regularizer = %.3f\" % (N, alpha_list[i])) \n",
    "    ax.set_ylabel('MSE', fontsize=10)\n",
    "    ax.legend()\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.9])\n",
    "plt.savefig('MNIST_FFNN_accuracy_ex1.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_accuracy_metrics(Y_test, P_pred, class_labels=None, use_opt_threshold=False):\n",
    "    # y_test = multiclass one-hot encoding  labels \n",
    "    # Q = predicted probability for y_test\n",
    "    # compuate various classification accuracy metrics\n",
    "    results_dict = {}\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for i in np.arange(Y_test.shape[0]):\n",
    "        for j in np.arange(Y_test.shape[1]):\n",
    "            if Y_test[i,j] == 1:\n",
    "                y_test.append(j)\n",
    "            if P_pred[i,j] == np.max(P_pred[i,:]):\n",
    "                # print('!!!', np.where(P_pred[i,:]==np.max(P_pred[i,:])))\n",
    "                y_pred.append(j)\n",
    "            \n",
    "    confusion_mx = metrics.confusion_matrix(y_test, y_pred)\n",
    "    results_dict.update({'confusion_mx':confusion_mx})\n",
    "    results_dict.update({'Accuracy':np.trace(confusion_mx)/np.sum(np.sum(confusion_mx))})\n",
    "    print('!!! confusion_mx', confusion_mx)\n",
    "    print('!!! Accuracy', results_dict.get('Accuracy'))\n",
    "    \n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit multiclass logistic regression using GD\n",
    "\n",
    "list_digits=['0', '1', '2']\n",
    "X_train, X_test, y_train, y_test = sample_multiclass_MNIST(list_digits=list_digits, full_MNIST = [X,y])\n",
    "\n",
    "NN = FFNN(list_hidden_layer_sizes = [100], # hidden1, hidden2, .. , hidden h\n",
    "                     loss_function = 'softmax-cross-entropy', # 'cross-entropy' or 'square'\n",
    "                     activation_list = None, # ['ReLU', 'sigmoid'],\n",
    "                     node_states = None,\n",
    "                     weight_matrices = None,\n",
    "                     training_set = [X_train.T/100, y_train.T]) # input = [N x ], output \n",
    "\n",
    "NN.train(n_SGD_iter=40, minibatch_size=100, stopping_diff=0.01, verbose=False, L2_reg = 0)\n",
    "y_pred_train = np.asarray(NN.predict(X_train.T/100, normalize=True))\n",
    "y_pred_test = np.asarray(NN.predict(X_test.T/100, normalize=True))\n",
    "\n",
    "print('y_pred_test.shape', y_pred_test.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "#y_train_label = np.asarray(onehot2list(y_train))\n",
    "#y_test_label = np.asarray(onehot2list(y_test))\n",
    "\n",
    "P_pred_train = np.asarray([p[1] for p in y_pred_train])\n",
    "P_pred_test = np.asarray([p[1] for p in y_pred_test])\n",
    "\n",
    "results_dict_train = multiclass_accuracy_metrics(Y_test=y_train, P_pred=y_pred_train, use_opt_threshold=False)\n",
    "results_dict_test = multiclass_accuracy_metrics(Y_test=y_test, P_pred=y_pred_test, use_opt_threshold=False)\n",
    "accuracy_list_test.append(results_dict_train.get('Accuracy'))\n",
    "accuracy_list_train.append(results_dict_test.get('Accuracy'))\n",
    "\n",
    "\"\"\"\n",
    "# make plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(list_digits), figsize=[12, 4])\n",
    "for i in np.arange(len(list_digits)):\n",
    "    L = list_digits[i]\n",
    "    im = ax[i].imshow(W[1:,i].reshape(28,28), vmin=np.min(W), vmax=np.max(W))\n",
    "    ax[i].title.set_text(\"MLR coeff. for %s\" % L )\n",
    "    # ax[i].legend()\n",
    "    # if i == len(list_digits) - 1:\n",
    "    \n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig('MLR_MNIST_ex1.pdf', bbox_inches='tight')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.random.rand(5,4)\n",
    "X_ext = np.vstack((X_train, np.ones(X_train.shape[1])))\n",
    "X_ext[:-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN():\n",
    "\n",
    "    def __init__(self,\n",
    "                 list_hidden_layer_sizes = [30], # hidden1, hidden2, .. , hidden h\n",
    "                 loss_function = 'softmax-cross-entropy', # or 'square' or 'cross-entropy' or 'identity'\n",
    "                 activation_list = None, # ['ReLU', 'sigmoid'],\n",
    "                 node_states = None,\n",
    "                 weight_matrices = None,\n",
    "                 training_set = [None, None]): # input = [feature_dim x samples], output [\\kappa x samples]\n",
    "                 \n",
    "        self.training_set = training_set\n",
    "        # self.test_set = test_set\n",
    "        self.list_layer_sizes = [self.training_set[0].shape[0]] + list_hidden_layer_sizes + [self.training_set[1].shape[0]]        \n",
    "        self.list_layer_sizes[0] += 1 # add hidden unit in the input layer. For hidden layers, the last node are the hidden units.    \n",
    "        self.n_layers = len(self.list_layer_sizes)-1\n",
    "        self.loss_function = loss_function\n",
    "        self.activation_list = activation_list\n",
    "        self.node_states = node_states\n",
    "        self.weight_matrices = weight_matrices\n",
    "        \n",
    "        self.initialize()\n",
    "        \n",
    "       \n",
    "    def initialize(self):\n",
    "        if self.activation_list is None:\n",
    "            activation_list = ['ReLU' for i in np.arange(len(self.list_layer_sizes)-1)]\n",
    "            activation_list[-1] = 'sigmoid'\n",
    "            self.activation_list = activation_list\n",
    "\n",
    "        if self.node_states == None:\n",
    "            node_states = []\n",
    "            for i in np.arange(len(self.list_layer_sizes)):\n",
    "                node_states.append(np.zeros(shape=[self.list_layer_sizes[i], ]))\n",
    "            self.node_states = node_states\n",
    "        \n",
    "        if self.weight_matrices == None:\n",
    "            weight_matrices = []\n",
    "            for i in np.arange(len(self.activation_list)):\n",
    "                U = np.random.rand(self.list_layer_sizes[i], self.list_layer_sizes[i+1]) \n",
    "                weight_matrices.append(1-2*U)\n",
    "            self.weight_matrices = weight_matrices\n",
    "            print('weight_matrix.shape', U.shape)\n",
    "\n",
    "       \n",
    "    def forward_propagate(self, input_data):\n",
    "        # Forward propagate the input using the current weights and update node states \n",
    "        self.node_states[0] = np.append(input_data, [1]) # 1 for hidden unit \n",
    "        for i in np.arange(self.n_layers):    \n",
    "            X_new = self.node_states[i].T @ self.weight_matrices[i]\n",
    "            X_new = activation(X_new, type=self.activation_list[i])\n",
    "            if i < self.n_layers-1:\n",
    "                X_new[-1]=1 # reset state of the hidden units to 1 (output layer has no hidden unit)\n",
    "            self.node_states[i+1] = X_new\n",
    "        \n",
    "    def backpropagate(self, output_data):\n",
    "        # Backpropagate the error and return the gradient of the weight matrices\n",
    "        # output_data = column array\n",
    "        node_errors = self.node_states.copy()\n",
    "\n",
    "        y = output_data\n",
    "        y_hat = self.node_states[-1] # shape (\\kappa, )\n",
    "        y_hat = y_hat[:, np.newaxis]\n",
    "        node_errors[-1] = delta_loss_function(y=y, y_hat=y_hat, type=self.loss_function)\n",
    "        W_grad = self.weight_matrices.copy()\n",
    "        \n",
    "        for i in range(self.n_layers -1, -1, -1):\n",
    "            # First weight the errors of nodes in layer above by the derivative of activation\n",
    "            wtd_errors = node_errors[i+1].copy() \n",
    "            z = self.node_states[i][:,np.newaxis]\n",
    "            W = self.weight_matrices[i]\n",
    "            layer_size_above = self.list_layer_sizes[i+1]\n",
    "            delta_activation_weights = [delta_activation(z.T @ W[:,q], type=self.activation_list[i]) for q in np.arange(layer_size_above)]\n",
    "            delta_activation_weights = np.asarray(delta_activation_weights)\n",
    "            if len(delta_activation_weights.shape)==1:\n",
    "                delta_activation_weights = delta_activation_weights[:,np.newaxis] \n",
    "            if len(wtd_errors.shape)==1:\n",
    "                wtd_errors = wtd_errors[:,np.newaxis] \n",
    "        \n",
    "            wtd_errors = wtd_errors * delta_activation_weights\n",
    "            # wtd_errors = wtd_errors[:, np.newaxis]\n",
    "\n",
    "            # Compute the gradient of the i th weight matrix (conneting layer i and i+1)\n",
    "            W_grad[i] = wtd_errors @ z.T  \n",
    "            \n",
    "            # Propagate it backward onto layer i\n",
    "            node_errors[i] = (W @ wtd_errors)[:,0]\n",
    "        return W_grad\n",
    "    \n",
    "    def minibatch_grad(self, minibatch_idx, L2_reg=0):\n",
    "        \n",
    "        W_grad_list = []\n",
    "        Y = self.training_set[1] # true labels: each column = one-hot encoding\n",
    "        X = self.training_set[0]\n",
    "        for i in minibatch_idx:\n",
    "            self.forward_propagate(input_data=X[:,i])\n",
    "            y = Y[:, i]\n",
    "            y = y[:, np.newaxis]\n",
    "            W_grad = self.backpropagate(output_data=y)\n",
    "            W_grad_list.append(W_grad)\n",
    "            \n",
    "        W_grad_minibatch = self.weight_matrices.copy()\n",
    "        for j in np.arange(self.n_layers):\n",
    "            grad_temp = [W_grad_list[i][j] for i in np.arange(len(minibatch_idx))]\n",
    "            W_grad_minibatch[j] = np.sum(np.asarray(grad_temp), axis=0).T + L2_reg * self.weight_matrices[j]\n",
    "            \n",
    "        return W_grad_minibatch \n",
    "        \n",
    "    def train(self, n_SGD_iter=10, minibatch_size=1, stopping_diff=0.01, verbose=False, L2_reg=0):\n",
    "        Y = self.training_set[1]\n",
    "        weight_matrices_new = self.weight_matrices\n",
    "        for i in np.arange(n_SGD_iter):\n",
    "            # compute the minibatch gradients of weight matrices \n",
    "            num_train_data = Y.shape[0]\n",
    "            minibatch_idx = np.random.choice(np.arange(num_train_data), minibatch_size)\n",
    "            W_grad_minibatch = self.minibatch_grad(minibatch_idx=minibatch_idx, L2_reg=L2_reg)\n",
    "            \n",
    "            # GD \n",
    "            \n",
    "            y_hat = np.asarray(self.predict(self.training_set[0])).T\n",
    "            X_train = self.training_set[0].copy()\n",
    "            print('!! y_hat.shape', y_hat.shape)\n",
    "            # print('!! X_train', X_train)\n",
    "            \n",
    "            for j in np.arange(self.n_layers):\n",
    "                W1 = self.weight_matrices[j]\n",
    "                t = 0\n",
    "                grad = W_grad_minibatch[j]\n",
    "                grad_old = grad.copy()\n",
    "                \n",
    "                \n",
    "                if j == 1:\n",
    "                    grad = np.zeros(shape=W1.shape)\n",
    "                    \n",
    "                    for s in np.arange(Y.shape[1]):\n",
    "                        X = self.training_set[0]\n",
    "                        self.forward_propagate(input_data=X[:,s])\n",
    "                        print('!!y_hat', y_hat[:,s])\n",
    "                        print('!! y', Y[:,s])\n",
    "                        print('!! norm', np.linalg.norm(y_hat[:,s] - Y[:,s])**2)\n",
    "                        a = (y_hat[:,s] - Y[:,s]).reshape(-1,1)\n",
    "                        b = self.node_states[1].reshape(1,-1)\n",
    "                        print('!! b', b)\n",
    "                        grad = a @ b\n",
    "                        print('grad', grad)\n",
    "                        grad = grad.T\n",
    "                        print('!!! iter', i)\n",
    "                        step = (np.log(i+2) / (((i + 1) ** (0.5))))\n",
    "                        W1  -= step * grad\n",
    "                    # print('grad_old-grad', np.linalg.norm(grad_old-grad))\n",
    "                \n",
    "                if j == 0:\n",
    "                    grad = np.zeros(shape=W1.shape)\n",
    "                    W2 = self.weight_matrices[1]\n",
    "                    for s in np.arange(Y.shape[1]):\n",
    "                        X = self.training_set[0]\n",
    "                        self.forward_propagate(input_data=X[:,s])\n",
    "                        x = np.append(X_train[:,s],[1]).reshape(-1,1)\n",
    "                        for row in np.arange(W1.shape[0]):\n",
    "                            for col in np.arange(W1.shape[1]):\n",
    "                                grad[row,col] += (W2[col,:]@(y_hat[:,s] - Y[:,s]))*(1-tanh(x.T @ W1[:,col])**2)*x[row,0]\n",
    "                    # print('grad_old-grad', np.linalg.norm(grad_old-grad))\n",
    "                    \n",
    "                # if (np.linalg.norm(grad) > stopping_diff):\n",
    "                        step = (np.log(i+2) / (((i + 1) ** (0.5))))\n",
    "                        W1 -= step * grad\n",
    "                    \n",
    "                print('!!! W change', np.linalg.norm(self.weight_matrices[j]-W1))\n",
    "            \n",
    "                self.weight_matrices[j] = W1.copy()\n",
    "                \n",
    "                \n",
    "                if (j == 0) and verbose:\n",
    "                    y_hat = np.asarray(self.predict(self.training_set[0])).T\n",
    "                    train_loss = np.sum(loss_function(y=Y, y_hat=y_hat, type=self.loss_function))\n",
    "                    # print('y_hat', y_hat)\n",
    "                    print('SGD epoch = %i, train_loss=%f, grad_norm = %f' %(i, train_loss, np.linalg.norm(grad)))\n",
    "                    #print('SGD epoch = %i, grad_norm = %f' %(i, np.linalg.norm(grad)))\n",
    "                   \n",
    "                    #y_train = np.asarray(onehot2list(Y.T))\n",
    "                    #P_pred = y_hat[1,:]\n",
    "                    \n",
    "                    # print('Y-y_hat', 2*(np.linalg.norm(Y-y_hat))**2)\n",
    "                    \n",
    "                    # print('grad', grad.T)\n",
    "                    \n",
    "                    #metrics = compute_accuracy_metrics(Y_test=y_train, P_pred=P_pred, use_opt_threshold=False, verbose=False)\n",
    "                    #print('Confusion matrix:\\n', metrics.get('Confusion_mx') )\n",
    "        \n",
    "            \n",
    "    def predict(self, test_set, normalize=False):\n",
    "        y_pred = []\n",
    "        for i in np.arange(test_set.shape[1]):\n",
    "            self.forward_propagate(input_data=test_set[:,i])\n",
    "            y_hat = self.node_states[-1].copy()\n",
    "            if normalize:\n",
    "                y_hat /= np.sum(y_hat)\n",
    "            y_pred.append(y_hat)\n",
    "            # print('!! y_hat', y_hat)\n",
    "        return y_pred\n",
    "            \n",
    "        \n",
    "### Helper functions\n",
    "\n",
    "def loss_function(y, y_hat, type='cross-entropy'):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    if type == 'cross_entropy':\n",
    "        return cross_entropy(y=y, y_hat=y_hat)\n",
    "    elif type == 'square':\n",
    "        return (1/2) * (y_hat - y).T @ (y_hat - y)\n",
    "    elif type == 'softmax-cross-entropy':\n",
    "        return cross_entropy(y=y, y_hat=softmax(y_hat))\n",
    "   \n",
    "\n",
    "def delta_loss_function(y, y_hat, type='cross-entropy'):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    # return delta_cross_entropy(y=y, y_hat=y_hat/np.sum(y_hat))\n",
    "    \n",
    "    if type == 'cross-entropy':\n",
    "        return delta_cross_entropy(y=y, y_hat=y_hat)\n",
    "    elif type == 'square':\n",
    "        return y_hat - y\n",
    "    elif type == 'softmax-cross-entropy':\n",
    "        return softmax(y_hat) - y\n",
    "\n",
    "        \n",
    "def activation(x, type='sigmoid'):\n",
    "    if type == 'sigmoid':\n",
    "        return 1/(1+np.exp(-x))\n",
    "    elif type == 'ReLU':\n",
    "        return np.maximum(0,x)\n",
    "    elif type == 'tanh':\n",
    "        return tanh(x)\n",
    "    elif type == 'identity':\n",
    "        return x\n",
    "    \n",
    "def delta_activation(x, type='sigmoid'):\n",
    "    # derivate of activation function\n",
    "    if type == 'sigmoid':\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    elif type == 'ReLU':\n",
    "        return int((x>0))\n",
    "    elif type == 'tanh':\n",
    "        return 1-(tanh(x))**2\n",
    "    elif type == 'identity':\n",
    "        return 1\n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def tanh(x):\n",
    "    #return (1-np.exp(-2*x))/(1+np.exp(-2*x))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    return -(y.T @ np.log(y_hat))[0][0]\n",
    "\n",
    "def delta_cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    y_hat = column array of predictive PMF \n",
    "    y = column array of one-hot encoding of true class label\n",
    "    \"\"\"\n",
    "    y_hat /= np.max(y_hat)\n",
    "    z = y.copy()\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        a = y.argmax(axis=0)[0]\n",
    "        z[i,0] = -1/y_hat[a, 0]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \n",
    "    # multilayer perceptron with tanh and identity activation for the hidden and output layers \n",
    "    # train with square loss \n",
    "    # M = # of hidden nodes\n",
    "    # training_set = [X_train, Y_train]\n",
    "    # X_train.shape = [features x samples]\n",
    "    # Y_train.shape = [features x samples]\n",
    "    \n",
    "    def __init__(self,\n",
    "                 M = 30, # number of hidden nodes (except hidden unit)\n",
    "                 training_set = [None, None]): # input = [feature_dim x samples], output [\\kappa x samples]\n",
    "        \n",
    "        self.M = M\n",
    "        self.X_train = training_set[0]\n",
    "        self.Y_train = training_set[1]\n",
    "        \n",
    "        print('X_train.shape', self.X_train.shape)\n",
    "        print('Y_train.shape', self.Y_train.shape)\n",
    "        \n",
    "        \n",
    "        # Initialize weight matrices \n",
    "        X_ext = np.vstack((self.X_train, np.ones(self.X_train.shape[1]))) # add 1 for hidden units in the input layer\n",
    "        print('X_ext.shape', X_ext.shape)\n",
    "        W0 = np.random.rand(self.X_train.shape[0]+1, self.M) # add one more row for bias, extra column is all zeros \n",
    "        W1 = np.random.rand(self.M+1, self.Y_train.shape[0]) # add one more row for bias\n",
    "        print('W0.shape', W0.shape)\n",
    "        print('W1.shape', W1.shape)\n",
    "        \n",
    "        self.weight_matrices = [W0, W1]\n",
    "        \n",
    "    def train(self, n_iter=10):\n",
    "        W0 = self.weight_matrices[0]\n",
    "        W1 = self.weight_matrices[1]\n",
    "        \n",
    "        for t in np.arange(n_iter):\n",
    "            # SGD optimization \n",
    "            X_ext = np.vstack((self.X_train, np.ones(self.X_train.shape[1])))\n",
    "            step = (1 / (np.log(t+2) * ((t + 10) ** (0.5))))\n",
    "            grad = self.total_grad(X_ext, self.Y_train, W0, W1)\n",
    "            W0  -= step * grad[0] # do not update the last column (hidden unit --> next layer)\n",
    "            W1  -= step * grad[1] # do not update the last column(hidden unit --> next layer)\n",
    "\n",
    "            self.weight_matrices = [W0, W1]\n",
    "\n",
    "            # calculate error\n",
    "            y_hat = self.predict(self.X_train)\n",
    "            y = self.Y_train\n",
    "\n",
    "            error = 0.0\n",
    "            for k in range(y_hat.shape[1]):\n",
    "                error += 0.5 * np.linalg.norm(y_hat[:,k] - y[:,k])**2 \n",
    "\n",
    "            print('SGD epoch = %i, train_loss=%f' %(t, error))\n",
    "        \n",
    "        return [W0, W1]\n",
    "\n",
    "    def per_sample_grad(self, x_train, y_train, W0, W1):\n",
    "        # forward_propagate \n",
    "        x = x_train # input states  \n",
    "        z = tanh(x.T @ W0).T # hidden states \n",
    "        z_ext = np.vstack((z, [1])) # add the hidden unit\n",
    "        y_hat = sigmoid(z_ext.T @ W1).T # output states\n",
    "        grad1 = (z_ext @ ((y_hat - y_train) * dsigmoid(W1.T @ z_ext)).T) \n",
    "           \n",
    "        a = (W1[:-1,:] @ (y_hat - y_train))\n",
    "        b = (1-z**2)  \n",
    "        grad0 = x @ (a*b).T \n",
    "        return [grad0, grad1] \n",
    "    \n",
    "    def total_grad(self, X_train, Y_train, W0, W1):\n",
    "        grad0 = np.zeros(shape = W0.shape)\n",
    "        grad1 = np.zeros(shape = W1.shape)\n",
    "        for s in np.arange(X_train.shape[1]):\n",
    "            x = X_train[:,s]\n",
    "            y = Y_train[:,s]\n",
    "            per_sample_grad = self.per_sample_grad(x[:,np.newaxis], y[:,np.newaxis], W0, W1)\n",
    "            grad0 += per_sample_grad[0]\n",
    "            grad1 += per_sample_grad[1]\n",
    "        return [grad0, grad1]\n",
    "    \n",
    "            \n",
    "    def predict(self, X_test, normalize = False):\n",
    "        X_ext = np.vstack((X_test, np.ones(X_test.shape[1]))) # add 1 for hidden units in the input layer\n",
    "        # forward_propagate \n",
    "        W0 = self.weight_matrices[0]\n",
    "        W1 = self.weight_matrices[1]\n",
    "        x = X_ext # input states\n",
    "        z = tanh(x.T @ W0).T # hidden states \n",
    "        z = np.vstack((z, np.ones(z.shape[1]))) # add the hidden unit\n",
    "        y_hat = sigmoid((z.T @ W1).T) # output states\n",
    "        \n",
    "        if normalize:\n",
    "            sum_of_cols = y_hat.sum(axis=0)\n",
    "            y_hat /= sum_of_cols[np.newaxis,:]\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fitting using FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_poly_matrix(x, deg=5):\n",
    "    # given a d-dim vector x = [x1, x2, .. xd], make a d x (M+1) matrix \n",
    "    # 1 x1 x1**2 ... x1**M\n",
    "    # 1 x2 x2**2 ... x2**M\n",
    "    # ..\n",
    "    # 1 xd xd**2 ... xd**M\n",
    "    x = np.asarray(x)\n",
    "    X = [np.ones(len(x))]\n",
    "    x1 = x.copy()\n",
    "    for i in np.arange(deg):\n",
    "        X.append(x1)\n",
    "        x1 = x1 * x\n",
    "\n",
    "    X = np.asarray(X).T # d x M matrix of input \n",
    "    # print(X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_regression(x_train, y_train, deg=9, alpha=0):\n",
    "    # fit polynomial regression of given degree with L2 regularizer alpha\n",
    "    X = make_poly_matrix(x_train, deg=deg)\n",
    "    w_hat = np.linalg.pinv(X.T @ X + alpha*np.identity(X.shape[1])) @ X.T @ y_train\n",
    "    y_hat_train = X @ w_hat\n",
    "    return y_hat_train, w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_MLP(X_test, W0, W1):\n",
    "    X_ext = np.vstack((X_test, np.ones(X_test.shape[1]))) # add 1 for hidden units in the input layer\n",
    "    # forward_propagate \n",
    "    print('X_test.shape', X_test.shape)\n",
    "    print('W0.shape', W0.shape)\n",
    "    print('W1.shape', W1.shape)\n",
    "    \n",
    "    \n",
    "    x = X_ext # input states\n",
    "    z = tanh(x.T @ W0).T # hidden states \n",
    "    z = np.vstack((z, np.ones(z.shape[1]))) # add the hidden unit\n",
    "    y_hat = (z.T @ W1).T # output states\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape (10,)\n",
      "y_train.shape (10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa04ab45580>"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate train and test data from y = sin(2\\pi x) + N(0,\\sigma^2)\n",
    "\n",
    "np.random.seed(0)\n",
    "N = 10\n",
    "sigma = 1\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in np.arange(N):\n",
    "    U1 = np.random.rand()\n",
    "    U2 = np.random.rand()\n",
    "    e1 = np.random.normal(0,sigma**2)\n",
    "    e2 = np.random.normal(0,sigma**2) \n",
    "    x_train.append(U1)\n",
    "    x_test.append(U2)\n",
    "    y_train.append(np.sin(2*np.pi*U1)+e1)\n",
    "    y_test.append(np.sin(2*np.pi*U2)+e2)\n",
    "    \n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# make plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.plot(x_train, y_train, 'o', color='blue', label=\"train data\")\n",
    "x = np.linspace(0, 1, 100)\n",
    "ax.plot(x, np.sin(2*np.pi*x), color='lightgreen', label=\"$y=\\sin(2\\pi x)$\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (1, 10)\n",
      "Y_train.shape (1, 10)\n",
      "X_ext.shape (2, 10)\n",
      "W0.shape (2, 1)\n",
      "W1.shape (2, 1)\n"
     ]
    }
   ],
   "source": [
    "M = 1\n",
    "MM = MLP(M = M, training_set = [x_train[np.newaxis,:], y_train[np.newaxis,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD epoch = 0, train_loss=9.930311\n",
      "train_loss=%f 9.930310547631604\n",
      "SGD epoch = 1, train_loss=9.925217\n",
      "train_loss=%f 9.925216967134594\n",
      "SGD epoch = 2, train_loss=9.921553\n",
      "train_loss=%f 9.921553184276167\n",
      "SGD epoch = 3, train_loss=9.918620\n",
      "train_loss=%f 9.91862033168566\n",
      "SGD epoch = 4, train_loss=9.916139\n",
      "train_loss=%f 9.916139334729396\n",
      "SGD epoch = 5, train_loss=9.913969\n",
      "train_loss=%f 9.913968695749885\n",
      "SGD epoch = 6, train_loss=9.912026\n",
      "train_loss=%f 9.912025910454082\n",
      "SGD epoch = 7, train_loss=9.910258\n",
      "train_loss=%f 9.910258227922274\n",
      "SGD epoch = 8, train_loss=9.908630\n",
      "train_loss=%f 9.908629643670599\n",
      "SGD epoch = 9, train_loss=9.907114\n",
      "train_loss=%f 9.907114358125169\n",
      "SGD epoch = 10, train_loss=9.905693\n",
      "train_loss=%f 9.905693176494884\n",
      "SGD epoch = 11, train_loss=9.904351\n",
      "train_loss=%f 9.904351385528159\n",
      "SGD epoch = 12, train_loss=9.903077\n",
      "train_loss=%f 9.903077430669986\n",
      "SGD epoch = 13, train_loss=9.901862\n",
      "train_loss=%f 9.901862054296934\n",
      "SGD epoch = 14, train_loss=9.900698\n",
      "train_loss=%f 9.900697713168176\n",
      "SGD epoch = 15, train_loss=9.899578\n",
      "train_loss=%f 9.899578172161233\n",
      "SGD epoch = 16, train_loss=9.898498\n",
      "train_loss=%f 9.898498213309667\n",
      "SGD epoch = 17, train_loss=9.897453\n",
      "train_loss=%f 9.897453422579538\n",
      "SGD epoch = 18, train_loss=9.896440\n",
      "train_loss=%f 9.896440030457908\n",
      "SGD epoch = 19, train_loss=9.895455\n",
      "train_loss=%f 9.895454790660642\n",
      "SGD epoch = 20, train_loss=9.894495\n",
      "train_loss=%f 9.894494886399388\n",
      "SGD epoch = 21, train_loss=9.893558\n",
      "train_loss=%f 9.893557856937656\n",
      "SGD epoch = 22, train_loss=9.892642\n",
      "train_loss=%f 9.892641539328261\n",
      "SGD epoch = 23, train_loss=9.891744\n",
      "train_loss=%f 9.891744021677301\n",
      "SGD epoch = 24, train_loss=9.890864\n",
      "train_loss=%f 9.890863605276113\n",
      "SGD epoch = 25, train_loss=9.889999\n",
      "train_loss=%f 9.889998773638162\n",
      "SGD epoch = 26, train_loss=9.889148\n",
      "train_loss=%f 9.889148166971323\n",
      "SGD epoch = 27, train_loss=9.888311\n",
      "train_loss=%f 9.888310560971844\n",
      "SGD epoch = 28, train_loss=9.887485\n",
      "train_loss=%f 9.887484849086091\n",
      "SGD epoch = 29, train_loss=9.886670\n",
      "train_loss=%f 9.886670027578393\n",
      "SGD epoch = 30, train_loss=9.885865\n",
      "train_loss=%f 9.88586518288738\n",
      "SGD epoch = 31, train_loss=9.885069\n",
      "train_loss=%f 9.88506948086203\n",
      "SGD epoch = 32, train_loss=9.884282\n",
      "train_loss=%f 9.88428215755204\n",
      "SGD epoch = 33, train_loss=9.883503\n",
      "train_loss=%f 9.8835025112914\n",
      "SGD epoch = 34, train_loss=9.882730\n",
      "train_loss=%f 9.882729895864086\n",
      "SGD epoch = 35, train_loss=9.881964\n",
      "train_loss=%f 9.881963714580115\n",
      "SGD epoch = 36, train_loss=9.881203\n",
      "train_loss=%f 9.881203415121323\n",
      "SGD epoch = 37, train_loss=9.880448\n",
      "train_loss=%f 9.880448485040889\n",
      "SGD epoch = 38, train_loss=9.879698\n",
      "train_loss=%f 9.87969844782067\n",
      "SGD epoch = 39, train_loss=9.878953\n",
      "train_loss=%f 9.878952859406343\n",
      "SGD epoch = 40, train_loss=9.878211\n",
      "train_loss=%f 9.87821130515339\n",
      "SGD epoch = 41, train_loss=9.877473\n",
      "train_loss=%f 9.8774733971277\n",
      "SGD epoch = 42, train_loss=9.876739\n",
      "train_loss=%f 9.876738771713217\n",
      "SGD epoch = 43, train_loss=9.876007\n",
      "train_loss=%f 9.876007087486355\n",
      "SGD epoch = 44, train_loss=9.875278\n",
      "train_loss=%f 9.875278023322851\n",
      "SGD epoch = 45, train_loss=9.874551\n",
      "train_loss=%f 9.87455127670774\n",
      "SGD epoch = 46, train_loss=9.873827\n",
      "train_loss=%f 9.873826562223314\n",
      "SGD epoch = 47, train_loss=9.873104\n",
      "train_loss=%f 9.873103610193457\n",
      "SGD epoch = 48, train_loss=9.872382\n",
      "train_loss=%f 9.872382165465655\n",
      "SGD epoch = 49, train_loss=9.871662\n",
      "train_loss=%f 9.871661986314578\n",
      "SGD epoch = 50, train_loss=9.870943\n",
      "train_loss=%f 9.870942843453202\n",
      "SGD epoch = 51, train_loss=9.870225\n",
      "train_loss=%f 9.870224519139226\n",
      "SGD epoch = 52, train_loss=9.869507\n",
      "train_loss=%f 9.869506806366257\n",
      "SGD epoch = 53, train_loss=9.868790\n",
      "train_loss=%f 9.868789508130316\n",
      "SGD epoch = 54, train_loss=9.868072\n",
      "train_loss=%f 9.868072436763615\n",
      "SGD epoch = 55, train_loss=9.867355\n",
      "train_loss=%f 9.86735541332841\n",
      "SGD epoch = 56, train_loss=9.866638\n",
      "train_loss=%f 9.86663826706457\n",
      "SGD epoch = 57, train_loss=9.865921\n",
      "train_loss=%f 9.865920834885403\n",
      "SGD epoch = 58, train_loss=9.865203\n",
      "train_loss=%f 9.865202960916704\n",
      "SGD epoch = 59, train_loss=9.864484\n",
      "train_loss=%f 9.864484496074757\n",
      "SGD epoch = 60, train_loss=9.863765\n",
      "train_loss=%f 9.863765297679379\n",
      "SGD epoch = 61, train_loss=9.863045\n",
      "train_loss=%f 9.863045229098613\n",
      "SGD epoch = 62, train_loss=9.862324\n",
      "train_loss=%f 9.862324159421961\n",
      "SGD epoch = 63, train_loss=9.861602\n",
      "train_loss=%f 9.861601963159476\n",
      "SGD epoch = 64, train_loss=9.860879\n",
      "train_loss=%f 9.860878519964281\n",
      "SGD epoch = 65, train_loss=9.860154\n",
      "train_loss=%f 9.860153714376304\n",
      "SGD epoch = 66, train_loss=9.859427\n",
      "train_loss=%f 9.859427435585307\n",
      "SGD epoch = 67, train_loss=9.858700\n",
      "train_loss=%f 9.85869957721147\n",
      "SGD epoch = 68, train_loss=9.857970\n",
      "train_loss=%f 9.857970037101916\n",
      "SGD epoch = 69, train_loss=9.857239\n",
      "train_loss=%f 9.857238717141835\n",
      "SGD epoch = 70, train_loss=9.856506\n",
      "train_loss=%f 9.856505523078843\n",
      "SGD epoch = 71, train_loss=9.855770\n",
      "train_loss=%f 9.855770364359524\n",
      "SGD epoch = 72, train_loss=9.855033\n",
      "train_loss=%f 9.855033153977052\n",
      "SGD epoch = 73, train_loss=9.854294\n",
      "train_loss=%f 9.854293808328991\n",
      "SGD epoch = 74, train_loss=9.853552\n",
      "train_loss=%f 9.853552247084378\n",
      "SGD epoch = 75, train_loss=9.852808\n",
      "train_loss=%f 9.852808393059403\n",
      "SGD epoch = 76, train_loss=9.852062\n",
      "train_loss=%f 9.852062172100908\n",
      "SGD epoch = 77, train_loss=9.851314\n",
      "train_loss=%f 9.851313512977093\n",
      "SGD epoch = 78, train_loss=9.850562\n",
      "train_loss=%f 9.850562347274892\n",
      "SGD epoch = 79, train_loss=9.849809\n",
      "train_loss=%f 9.84980860930343\n",
      "SGD epoch = 80, train_loss=9.849052\n",
      "train_loss=%f 9.849052236003132\n",
      "SGD epoch = 81, train_loss=9.848293\n",
      "train_loss=%f 9.84829316686002\n",
      "SGD epoch = 82, train_loss=9.847531\n",
      "train_loss=%f 9.847531343824798\n",
      "SGD epoch = 83, train_loss=9.846767\n",
      "train_loss=%f 9.846766711236388\n",
      "SGD epoch = 84, train_loss=9.845999\n",
      "train_loss=%f 9.84599921574952\n",
      "SGD epoch = 85, train_loss=9.845229\n",
      "train_loss=%f 9.84522880626618\n",
      "SGD epoch = 86, train_loss=9.844455\n",
      "train_loss=%f 9.844455433870463\n",
      "SGD epoch = 87, train_loss=9.843679\n",
      "train_loss=%f 9.843679051766777\n",
      "SGD epoch = 88, train_loss=9.842900\n",
      "train_loss=%f 9.84289961522097\n",
      "SGD epoch = 89, train_loss=9.842117\n",
      "train_loss=%f 9.842117081504304\n",
      "SGD epoch = 90, train_loss=9.841331\n",
      "train_loss=%f 9.841331409839981\n",
      "SGD epoch = 91, train_loss=9.840543\n",
      "train_loss=%f 9.840542561352095\n",
      "SGD epoch = 92, train_loss=9.839750\n",
      "train_loss=%f 9.839750499016795\n",
      "SGD epoch = 93, train_loss=9.838955\n",
      "train_loss=%f 9.838955187615527\n",
      "SGD epoch = 94, train_loss=9.838157\n",
      "train_loss=%f 9.838156593690213\n",
      "SGD epoch = 95, train_loss=9.837355\n",
      "train_loss=%f 9.837354685500204\n",
      "SGD epoch = 96, train_loss=9.836549\n",
      "train_loss=%f 9.8365494329809\n",
      "SGD epoch = 97, train_loss=9.835741\n",
      "train_loss=%f 9.835740807703942\n",
      "SGD epoch = 98, train_loss=9.834929\n",
      "train_loss=%f 9.83492878283882\n",
      "SGD epoch = 99, train_loss=9.834113\n",
      "train_loss=%f 9.834113333115832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa07b5010a0>"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MM.train(n_iter = 100)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.plot(x_train, y_train, 'o', color='blue', label=\"train data\")\n",
    "x = np.linspace(-1, 1, 100)\n",
    "ax.plot(x, np.sin(2*np.pi*x), color='lightgreen', label=\"$y=\\sin(2\\pi x)$\")\n",
    "\n",
    "y_pred = MM.predict(x[np.newaxis,:])\n",
    "\n",
    "# y_pred = predict_MLP(x[np.newaxis,:], W0, W1)\n",
    "\n",
    "ax.plot(x, y_pred[0,:], color='red', label=\"$y=$FFNN prediction\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fitted polynomial regression curve\n",
    "deg_list = [1,4,9] # degree of polynomial \n",
    "alpha_list = [0, 0, 0.01, 0.1]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(deg_list), figsize=[16, 4])\n",
    "for i in np.arange(len(deg_list)):\n",
    "    # make plot\n",
    "    M = deg_list[i]\n",
    "    alpha = alpha_list[1]\n",
    "    ax[i].plot(x_train, y_train, 'o', color='blue', label=\"train data\")\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    ax[i].plot(x, np.sin(2*np.pi*x), color='lightgreen', label=\"$y=\\sin(2\\pi x)$\")\n",
    "    \n",
    "    NN_sin = FFNN(list_hidden_layer_sizes = [10], # hidden1, hidden2, .. , hidden h\n",
    "              activation_list = ['tanh', 'identity'], # For regression problems, should not use sigmoid in the last layer\n",
    "             loss_function = 'square', # or 'softmax-cross-entropy' or 'square'\n",
    "             training_set = [x_train[np.newaxis,:], y_train[np.newaxis,:]]) # input = [feature_dim x samples], output [\\kappa x samples]\n",
    "\n",
    "    NN_sin.train(n_SGD_iter=40, minibatch_size=100, stopping_diff=0.01, verbose=False)\n",
    "    y_pred = NN_sin.predict(x[np.newaxis,:], normalize=False)\n",
    "    # y_hat_train, w_hat = poly_regression(x_train, y_train, deg=M, alpha=alpha)\n",
    "    # ax.plot(x_train, y_hat_train, 'o', color = 'red')\n",
    "    # y_poly = make_poly_matrix(x, deg=M) @ w_hat\n",
    "    \n",
    "    ax[i].plot(x, y_pred, color = 'red', label=\"poly regression w/ deg %i\" % M)\n",
    "    ax[i].title.set_text(\"num training ex = %i, \\n L2 regularizer = %.3f\" % (N, alpha)) \n",
    "    ax[i].legend()\n",
    "    plt.savefig('poly_fitting_ex_2.pdf', bbox_inches='tight')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colourgraphenv",
   "language": "python",
   "name": "colourgraphenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
