{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils.extmath import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and display MNIST handwritten digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9a5d820bf25f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data from https://www.openml.org/d/554\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_784'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# X = X.values  ### Uncomment this line if you are having type errors in plotting. It is loading as a pandas dataframe, but our indexing is for numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame)\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;31m# obtain the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DATA_FILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_description\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m     bunch = _download_data_to_bunch(url, return_sparse, data_home,\n\u001b[0m\u001b[1;32m    817\u001b[0m                                     \u001b[0mas_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                                     \u001b[0mfeatures_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, features_list, data_columns, target_columns, shape)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnominal_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     out = _retry_with_clean_cache(url, data_home)(\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0m_load_arff_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                              \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, return_type, encode_nominal, parse_arff)\u001b[0m\n\u001b[1;32m    464\u001b[0m                           \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                           encode_nominal=encode_nominal)\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparse_arff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mparse_arff\u001b[0;34m(arff)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mparse_arff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_arff_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_slice_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_slice_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0;31m# nominal attributes is a dict mapping from the attribute name to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;31m# the possible values. Includes also the target column (which will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_convert_arff_data\u001b[0;34m(arff, col_slice_x, col_slice_y, shape)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         data = np.fromiter(itertools.chain.from_iterable(arff_data),\n\u001b[0m\u001b[1;32m    253\u001b[0m                            dtype='float64', count=count)\n\u001b[1;32m    254\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36mdecode_rows\u001b[0;34m(self, stream, conversors)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_line\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# Note that if the data is dense, no reading is done until the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# generator is iterated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         arff = _arff.load((line.decode('utf-8') for line in response),\n\u001b[0m\u001b[1;32m    464\u001b[0m                           \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                           encode_nominal=encode_nominal)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    499\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0muncompress\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncompress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muncompress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/colourgraphenv/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36m_add_read_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "# X = X.values  ### Uncomment this line if you are having type errors in plotting. It is loading as a pandas dataframe, but our indexing is for numpy array. \n",
    "X = X / 255.\n",
    "\n",
    "print('X.shape', X.shape)\n",
    "print('y.shape', y.shape)\n",
    "\n",
    "'''\n",
    "Each row of X is a vectroization of an image of 28 x 28 = 784 pixels.  \n",
    "The corresponding row of y holds the true class label from {0,1, .. , 9}.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many images are there for each digit\n",
    "for j in np.arange(10):\n",
    "    idx = np.where(y==str(j))\n",
    "    idx = np.asarray(idx)[0,:]\n",
    "    print('digit %i length %i' % (j, len(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some sample images \n",
    "ncols = 10\n",
    "nrows = 4\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=[15, 6.5])\n",
    "for j in np.arange(ncols):\n",
    "    for i in np.arange(nrows):\n",
    "        idx = np.where(y==str(j)) # index of all images of digit 'j'\n",
    "        idx = np.asarray(idx)[0,:] # make idx from tuple to array\n",
    "        idx_subsampled = np.random.choice(idx, nrows)\n",
    "        ax[i,j].imshow(X[idx_subsampled[i],:].reshape(28,28))\n",
    "        # ax[i,j].title.set_text(\"label=%s\" % y[idx_subsampled[j]]) \n",
    "        if i == 0:\n",
    "            # ax[j,i].set_ylabel(\"label=%s\" % y[idx_subsampled[j]]) \n",
    "            ax[i,j].set_title(\"label$=$%s\" % y[idx_subsampled[i]], fontsize=14) \n",
    "        # ax[i].legend()\n",
    "plt.subplots_adjust(wspace=0.3, hspace=-0.1)\n",
    "plt.savefig('MNIST_ex1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "y_train = []\n",
    "for i in np.arange(X.shape[0]):\n",
    "    # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "    U = np.random.rand() # Uniform([0,1]) variable\n",
    "    if U<0.8:\n",
    "        X_train.append(X[i,:])\n",
    "        y_train.append(y[i])\n",
    "    else:\n",
    "        X_test.append(X[i,:])\n",
    "        y_test.append(y[i])\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_binary_MNIST(list_digits=['0','1'], full_MNIST=None, noise_rate=0):\n",
    "    # get train and test set from MNIST of given two digits\n",
    "    # e.g., list_digits = ['0', '1']\n",
    "    if full_MNIST is not None:\n",
    "        X, y = full_MNIST\n",
    "    else:\n",
    "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "        X = X / 255.\n",
    "\n",
    "    idx = [i for i in np.arange(len(y)) if y[i] in list_digits] # list of indices where the label y is in list_digits\n",
    "    \n",
    "    X01 = X[idx,:]\n",
    "    y01 = y[idx]\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_test = [] # list of integers 0 and 1s\n",
    "    y_train = [] # list of integers 0 and 1s\n",
    "\n",
    "    for i in np.arange(X01.shape[0]):\n",
    "        # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "        U = np.random.rand() # Uniform([0,1]) variable\n",
    "        label = 0\n",
    "        if y01[i] == str(list_digits[1]):\n",
    "            label = 1\n",
    "\n",
    "        if U<0.8:\n",
    "            # add noise to the sampled images \n",
    "            if noise_rate > 0:\n",
    "                for j in np.arange(X01.shape[1]):\n",
    "                    U1 = np.random.rand()\n",
    "                    if U1 < noise_rate:\n",
    "                        X01[i,j] += np.random.rand()\n",
    "                \n",
    "            X_train.append(X01[i,:])\n",
    "            y_train.append(label)\n",
    "        else:\n",
    "            X_test.append(X01[i,:])\n",
    "            y_test.append(label)\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_train = np.asarray(y_train).reshape(-1,1)\n",
    "    y_test = np.asarray(y_test).reshape(-1,1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = sample_binary_MNIST(list_digits=['0','1'], full_MNIST=[X, y], noise_rate=0.5)\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "print('y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot corrupted images \n",
    "ncols = 4\n",
    "fig, ax = plt.subplots(nrows=1, ncols=ncols, figsize=[15, 6.5])\n",
    "for j in np.arange(ncols):\n",
    "    id = np.random.choice(np.arange(X_train.shape[0]))\n",
    "    ax[j].imshow(X_train[id,:].reshape(28,28))\n",
    "    \n",
    "plt.savefig('MNIST_ex_corrupted1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2onehot(y, list_classes):\n",
    "    \"\"\"\n",
    "    y = list of class lables of length n\n",
    "    output = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    \"\"\"\n",
    "    Y = np.zeros(shape = [len(y), len(list_classes)], dtype=int)\n",
    "    for i in np.arange(Y.shape[0]):\n",
    "        for j in np.arange(len(list_classes)):\n",
    "            if y[i] == list_classes[j]:\n",
    "                Y[i,j] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_multiclass_MNIST(list_digits=['0','1', '2'], full_MNIST=None):\n",
    "    # get train and test set from MNIST of given digits\n",
    "    # e.g., list_digits = ['0', '1', '2']\n",
    "    if full_MNIST is not None:\n",
    "        X, y = full_MNIST\n",
    "    else:\n",
    "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "        X = X / 255.\n",
    "    Y = list2onehot(y.tolist(), list_digits)\n",
    "    \n",
    "    idx = [i for i in np.arange(len(y)) if y[i] in list_digits] # list of indices where the label y is in list_digits\n",
    "    \n",
    "    X01 = X[idx,:]\n",
    "    y01 = Y[idx,:]\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_test = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "    y_train = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "\n",
    "    for i in np.arange(X01.shape[0]):\n",
    "        # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "        U = np.random.rand() # Uniform([0,1]) variable\n",
    "        if U<0.8:\n",
    "            X_train.append(X01[i,:])\n",
    "            y_train.append(y01[i,:].copy())\n",
    "        else:\n",
    "            X_test.append(X01[i,:])\n",
    "            y_test.append(y01[i,:].copy())\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_train = np.asarray(y_train)\n",
    "    y_test = np.asarray(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# test \n",
    "X_train, X_test, y_train, y_test = sample_multiclass_MNIST(list_digits=['0','1', '2'], full_MNIST=[X, y])\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "print('y_test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid and logit function \n",
    "def sigmoid(x):\n",
    "    return np.exp(x)/(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sigmoid function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[10,3])\n",
    "x = np.linspace(-7, 7, 100)\n",
    "ax.plot(x, sigmoid(x), color='blue', label=\"$y=\\sigma(x)=\\exp(x)/(1+\\exp(x))$\")\n",
    "plt.axhline(y=1, color='g', linestyle='--')\n",
    "plt.axvline(x=0, color='g', linestyle='--')\n",
    "ax.legend()\n",
    "plt.savefig('sigmoid_ex.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_LR_GD(Y, H, W0=None, sub_iter=100, stopping_diff=0.01):\n",
    "        '''\n",
    "        Convex optimization algorithm for Logistic Regression using Gradient Descent \n",
    "        Y = (n x 1), H = (p x n) (\\Phi in lecture note), W = (p x 1)\n",
    "        Logistic Regression: Y ~ Bernoulli(Q), Q = sigmoid(H.T @ W)\n",
    "        MLE -->\n",
    "        Find \\hat{W} = argmin_W ( sum_j ( log(1+exp(H_j.T @ W) ) - Y.T @ H.T @ W ) )\n",
    "        '''\n",
    "        if W0 is None:\n",
    "            W0 = np.random.rand(H.shape[0],1) #If initial coefficients W0 is None, randomly initialize  \n",
    "            \n",
    "        W1 = W0.copy()\n",
    "        i = 0\n",
    "        grad = np.ones(W0.shape)\n",
    "        while (i < sub_iter) and (np.linalg.norm(grad) > stopping_diff):\n",
    "            Q = 1/(1+np.exp(-H.T @ W1))  # probability matrix, same shape as Y\n",
    "            # grad = H @ (Q - Y).T + alpha * np.ones(W0.shape[1])\n",
    "            grad = H @ (Q - Y)\n",
    "            W1 = W1 - (np.log(i+1) / (((i + 1) ** (0.5)))) * grad\n",
    "            i = i + 1\n",
    "            print('iter %i, grad_norm %f' %(i, np.linalg.norm(grad)))\n",
    "        return W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_LR_NR(Y, H, W0=None, sub_iter=100, stopping_diff=0.01):\n",
    "    '''\n",
    "    Convex optimization algorithm for Logistic Regression using Newton-Ralphson algorithm. \n",
    "    Y = (n x 1), H = (p x n) (\\Phi in lecture note), W = (p x 1)\n",
    "    Logistic Regression: Y ~ Bernoulli(Q), Q = sigmoid(H.T @ W)\n",
    "    MLE -->\n",
    "    Find \\hat{W} = argmin_W ( sum_j ( log(1+exp(H_j.T @ W) ) - Y.T @ H.T @ W ) ) \n",
    "    '''\n",
    "    ### Implement by yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit logistic regression using GD\n",
    "\n",
    "X_train, X_test, y_train, y_test = sample_binary_MNIST(['0', '1'], full_MNIST = [X,y])\n",
    "# Feature matrix of size (p x n) = (feature dim x samples)\n",
    "H_train = np.vstack((np.ones(X_train.shape[0]), X_train.T))  # add first row of 1's for bias features \n",
    "W = fit_LR_GD(Y=y_train, H=H_train/400) \n",
    "\n",
    "plt.imshow(W[1:,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fitted logistic regression curve\n",
    "digit_list_list = [['0','1'],['0','7'],['2','3'],['2', '8']] # list of list of two digits\n",
    "# fit LR for each cases\n",
    "W_array = []\n",
    "for i in np.arange(len(digit_list_list)):\n",
    "    L = digit_list_list[i]\n",
    "    X_train, X_test, y_train, y_test = sample_binary_MNIST(list_digits=L, full_MNIST = [X,y])\n",
    "    H_train = np.vstack((np.ones(X_train.shape[0]), X_train.T))  # add first row of 1's for bias features \n",
    "    W = fit_LR_GD(Y=y_train, H=H_train) \n",
    "    W = fit_LR_GD(Y=y_train, H=H_train) \n",
    "    W_array.append(W.copy())\n",
    "\n",
    "W_array = np.asarray(W_array)\n",
    "\n",
    "\n",
    "# make plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(digit_list_list), figsize=[16, 4])\n",
    "for i in np.arange(len(digit_list_list)):\n",
    "    L = digit_list_list[i]\n",
    "    W = W_array[i]\n",
    "    im = ax[i].imshow(W[1:,:].reshape(28,28), vmin=np.min(W_array), vmax=np.max(W_array))\n",
    "    ax[i].title.set_text(\"LR coeff. for %s vs. %s\" % (L[0], L[1])) \n",
    "    # ax[i].legend()\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig('LR_MNIST_training_ex.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(Y_test, P_pred, use_opt_threshold=False, verbose=False):\n",
    "    # y_test = binary label \n",
    "    # P_pred = predicted probability for y_test\n",
    "    # compuate various binary classification accuracy metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, P_pred, pos_label=None)\n",
    "    mythre = thresholds[np.argmax(tpr - fpr)]\n",
    "    myauc = metrics.auc(fpr, tpr)\n",
    "    # print('!!! auc', myauc)\n",
    "    \n",
    "    # Compute classification statistics\n",
    "    threshold = 0.5\n",
    "    if use_opt_threshold:\n",
    "        threshold = mythre\n",
    "    \n",
    "    Y_pred = P_pred.copy()\n",
    "    Y_pred[Y_pred < threshold] = 0\n",
    "    Y_pred[Y_pred >= threshold] = 1\n",
    "\n",
    "    mcm = confusion_matrix(Y_test, Y_pred)\n",
    "    tn = mcm[0, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    fp = mcm[0, 1]\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tn / (tn + fp)\n",
    "    specificity = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    fall_out = fp / (fp + tn)\n",
    "    miss_rate = fn / (fn + tp)\n",
    "\n",
    "    # Save results\n",
    "    results_dict = {}\n",
    "    results_dict.update({'Y_test': Y_test})\n",
    "    results_dict.update({'Y_pred': Y_pred})\n",
    "    results_dict.update({'AUC': myauc})\n",
    "    results_dict.update({'Opt_threshold': mythre})\n",
    "    results_dict.update({'Accuracy': accuracy})\n",
    "    results_dict.update({'Sensitivity': sensitivity})\n",
    "    results_dict.update({'Specificity': specificity})\n",
    "    results_dict.update({'Precision': precision})\n",
    "    results_dict.update({'Fall_out': fall_out})\n",
    "    results_dict.update({'Miss_rate': miss_rate})\n",
    "    \n",
    "    if verbose:\n",
    "        for key in [key for key in results_dict.keys()]:\n",
    "            print('% s ===> %.3f' % (key, results_dict.get(key)))\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit logistic regression using GD and compute binary classification accuracies\n",
    "\n",
    "# Get train and test data \n",
    "digits_list = ['4', '7']\n",
    "X_train, X_test, y_train, y_test = sample_binary_MNIST(digits_list, full_MNIST = [X,y])\n",
    "# Feature matrix of size (p x n) = (feature dim x samples)\n",
    "\n",
    "list_train_size = [1,10, 30, 100]\n",
    "\n",
    "# train the regression coefficients for all cases\n",
    "W_list = []\n",
    "results_list = []\n",
    "\n",
    "for i in np.arange(len(list_train_size)):\n",
    "    size = list_train_size[i]\n",
    "    idx = np.random.choice(np.arange(len(y_train)), size)\n",
    "    X_train0 = X_train[idx, :]\n",
    "    y_train0 = y_train[idx]\n",
    "\n",
    "    # Train the logistic regression model \n",
    "    H_train0 = np.vstack((np.ones(X_train0.shape[0]), X_train0.T))  # add first row of 1's for bias features \n",
    "    W = fit_LR_GD(Y=y_train0, H=H_train0) \n",
    "    W_list.append(W.copy()) # make sure use copied version of W since the same name is overrided in the loop \n",
    "\n",
    "    # Get predicted probabilities \n",
    "    H_test = np.vstack((np.ones(X_test.shape[0]), X_test.T))\n",
    "    Q = 1 / (1 + np.exp(-H_test.T @ W)) # predicted probabilities for y_test\n",
    "\n",
    "    # Compute binary classification accuracies\n",
    "    results_dict = compute_accuracy_metrics(Y_test=y_test, P_pred = Q)\n",
    "    results_dict.update({'train size':X_train0.shape[0]})  # add the train data size to the results dictionary\n",
    "    results_list.append(results_dict.copy())\n",
    "   \n",
    "    # Print out the results \n",
    "    \"\"\"\n",
    "    keys_list = [i for i in results_dict.keys()]\n",
    "    for key in keys_list:\n",
    "        if key not in ['Y_test', 'Y_pred']:\n",
    "            print('%s = %f' % (key, results_dict.get(key)))\n",
    "    \"\"\"\n",
    "\n",
    "# make plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(list_train_size), figsize=[16, 4])\n",
    "\n",
    "for i in np.arange(len(list_train_size)):\n",
    "    result_dict = results_list[i]\n",
    "    W = W_list[i][1:,:] \n",
    "    im = ax[i].imshow(W.copy().reshape(28,28), vmin=np.min(W_list), vmax=np.max(W_list))\n",
    "    \n",
    "    subtitle = \"\"\n",
    "    keys_list = [i for i in results_list[i].keys()]\n",
    "    for key in keys_list:\n",
    "        if key not in ['Y_test', 'Y_pred', 'AUC', 'Opt_threshold']:\n",
    "            subtitle += \"\\n\" + str(key) + \" = \" + str(np.round(results_list[i].get(key),3))\n",
    "            # print('%s = %f' % (key, results_list[i].get(key)))\n",
    "            \n",
    "    ax[i].set_title('Opt. regression coeff.', fontsize=13) \n",
    "    ax[i].set_xlabel(subtitle, fontsize=20) \n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "fig.suptitle(\"MNIST Binary Classification by LR for %s vs. %s\" % (digits_list[0], digits_list[1]), fontsize=20, y=1.05)\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig('LR_MNIST_test_ex1.pdf', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_MLR_GD(Y, H, W0=None, sub_iter=100, stopping_diff=0.01):\n",
    "        '''\n",
    "        Convex optimization algorithm for Multiclass Logistic Regression using Gradient Descent \n",
    "        Y = (n x k), H = (p x n) (\\Phi in lecture note), W = (p x k)\n",
    "        Multiclass Logistic Regression: Y ~ vector of discrete RVs with PMF = sigmoid(H.T @ W)\n",
    "        MLE -->\n",
    "        Find \\hat{W} = argmin_W ( sum_j ( log(1+exp(H_j.T @ W) ) - Y.T @ H.T @ W ) )\n",
    "        '''\n",
    "        k = Y.shape[1] # number of classes \n",
    "        if W0 is None:\n",
    "            W0 = np.random.rand(H.shape[0],k) #If initial coefficients W0 is None, randomly initialize  \n",
    "            \n",
    "        W1 = W0.copy()\n",
    "        i = 0\n",
    "        grad = np.ones(W0.shape)\n",
    "        while (i < sub_iter) and (np.linalg.norm(grad) > stopping_diff):\n",
    "            Q = 1/(1+np.exp(-H.T @ W1))  # probability matrix, same shape as Y\n",
    "            # grad = H @ (Q - Y).T + alpha * np.ones(W0.shape[1])\n",
    "            grad = H @ (Q - Y)\n",
    "            W1 = W1 - (np.log(i+1) / (((i + 1) ** (0.5)))) * grad\n",
    "            i = i + 1\n",
    "            # print('iter %i, grad_norm %f' %(i, np.linalg.norm(grad)))\n",
    "        return W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_softmax(a):\n",
    "    \"\"\"\n",
    "    given an array a = [a_1, .. a_k], compute the softmax distribution p = [p_1, .. , p_k] where p_i \\propto exp(a_i)\n",
    "    \"\"\"\n",
    "    a1 = a - np.max(a)\n",
    "    p = np.exp(a1)\n",
    "    if type(a) is list:\n",
    "        p = p/np.sum(p)\n",
    "    else: \n",
    "        row_sum = np.sum(p, axis=1)\n",
    "        p = p/row_sum[:, np.newaxis]\n",
    "    return p\n",
    "\n",
    "print(np.sum(custom_softmax([1,20,30,50])))\n",
    "a= np.ones((2,3))\n",
    "print(softmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_accuracy_metrics(Y_test, P_pred, class_labels=None, use_opt_threshold=False):\n",
    "    # y_test = multiclass one-hot encoding  labels \n",
    "    # Q = predicted probability for y_test\n",
    "    # compuate various classification accuracy metrics\n",
    "    results_dict = {}\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for i in np.arange(Y_test.shape[0]):\n",
    "        for j in np.arange(Y_test.shape[1]):\n",
    "            if Y_test[i,j] == 1:\n",
    "                y_test.append(j)\n",
    "            if P_pred[i,j] == np.max(P_pred[i,:]):\n",
    "                # print('!!!', np.where(P_pred[i,:]==np.max(P_pred[i,:])))\n",
    "                y_pred.append(j)\n",
    "            \n",
    "    confusion_mx = metrics.confusion_matrix(y_test, y_pred)\n",
    "    results_dict.update({'confusion_mx':confusion_mx})\n",
    "    results_dict.update({'Accuracy':np.trace(confusion_mx)/np.sum(np.sum(confusion_mx))})\n",
    "    print('!!! confusion_mx', confusion_mx)\n",
    "    print('!!! Accuracy', results_dict.get('Accuracy'))\n",
    "    \n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit multiclass logistic regression using GD\n",
    "\n",
    "list_digits=['0', '1', '2']\n",
    "X_train, X_test, y_train, y_test = sample_multiclass_MNIST(list_digits=list_digits, full_MNIST = [X,y])\n",
    "# Feature matrix of size (p x n) = (feature dim x samples)\n",
    "\n",
    "\n",
    "H_train = np.vstack((np.ones(X_train.shape[0]), X_train.T))  # add first row of 1's for bias features \n",
    "W = fit_MLR_GD(Y=y_train, H=H_train) \n",
    "print('!! W.shape', W.shape)\n",
    "\n",
    "# Get predicted probabilities \n",
    "H_test = np.vstack((np.ones(X_test.shape[0]), X_test.T))\n",
    "Q = softmax(H_test.T @ W.copy()) # predicted probabilities for y_test # Uses sklearn's softmax for numerical stability\n",
    "\n",
    "print('!!! y_test.shape', y_test.shape)\n",
    "\n",
    "print('!!! Q.shape', Q.shape)\n",
    "\n",
    "\n",
    "results_dict = multiclass_accuracy_metrics(Y_test=y_test, P_pred=Q)\n",
    "confusion_mx = results_dict.get('results_dict')\n",
    "\n",
    "# make plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(list_digits), figsize=[12, 4])\n",
    "for i in np.arange(len(list_digits)):\n",
    "    L = list_digits[i]\n",
    "    im = ax[i].imshow(W[1:,i].reshape(28,28), vmin=np.min(W), vmax=np.max(W))\n",
    "    ax[i].title.set_text(\"MLR coeff. for %s\" % L )\n",
    "    # ax[i].legend()\n",
    "    # if i == len(list_digits) - 1:\n",
    "    \n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig('MLR_MNIST_ex1.pdf', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit multiclass logistic regression using GD and compute multiclass classification accuracies\n",
    "\n",
    "# Get train and test data \n",
    "digits_list = ['0', '1', '2', '3', '4']\n",
    "X_train, X_test, y_train, y_test = sample_multiclass_MNIST(digits_list, full_MNIST = [X,y])\n",
    "\n",
    "# Feature matrix of size (p x n) = (feature dim x samples)\n",
    "\n",
    "list_train_size = [1,10, 30, 100]\n",
    "\n",
    "# train the regression coefficients for all cases\n",
    "W_list = []\n",
    "results_list = []\n",
    "\n",
    "for i in np.arange(len(list_train_size)):\n",
    "    size = list_train_size[i]\n",
    "    idx = np.random.choice(np.arange(len(y_train)), size)\n",
    "    X_train0 = X_train[idx, :]\n",
    "    y_train0 = y_train[idx, :]\n",
    "\n",
    "    # Train the multiclass logistic regression model \n",
    "    H_train0 = np.vstack((np.ones(X_train0.shape[0]), X_train0.T))  # add first row of 1's for bias features \n",
    "    W = fit_MLR_GD(Y=y_train0, H=H_train0) \n",
    "    W_list.append(W.copy()) # make sure use copied version of W since the same name is overrided in the loop \n",
    "\n",
    "    # Get predicted probabilities \n",
    "    H_test = np.vstack((np.ones(X_test.shape[0]), X_test.T))\n",
    "    Q = softmax(H_test.T @ W.copy()) # predicted probabilities for y_test # Uses sklearn's softmax for numerical stability\n",
    "\n",
    "    results_dict = multiclass_accuracy_metrics(Y_test=y_test, P_pred=Q)\n",
    "    results_dict.update({'train size':X_train0.shape[0]})  # add the train data size to the results dictionary\n",
    "    results_list.append(results_dict.copy())\n",
    "    \n",
    "# make plot\n",
    "fig, ax = plt.subplots(nrows=len(list_train_size), ncols=len(digits_list)+1, figsize=[15, 10])\n",
    "for i in np.arange(len(list_train_size)):\n",
    "    for j in np.arange(len(digits_list)+1):\n",
    "        if j < len(digits_list):\n",
    "            L = digits_list[j]\n",
    "            W = W_list[i]\n",
    "            im = ax[i,j].imshow(W[1:,j].reshape(28,28), vmin=np.min(W), vmax=np.max(W))\n",
    "            ax[i,j].title.set_text(\"MLR coeff. for %s\" % L )\n",
    "            if j == 0:\n",
    "                ax[i,j].set_ylabel(\"train size = %i\" % results_list[i].get(\"train size\"), fontsize=13)\n",
    "            divider = make_axes_locatable(ax[i,j])\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            fig.colorbar(im, cax=cax)\n",
    "        else:\n",
    "            confusion_mx = results_list[i].get(\"confusion_mx\")\n",
    "            im_confusion = ax[i,j].matshow(confusion_mx)\n",
    "            # ax[i,j].set_title(\"Confusion Matrix\")\n",
    "            ax[i,j].set_xlabel(\"Confusion Matrix\", fontsize=13)\n",
    "            # ax[i].legend()\n",
    "            # if i == len(list_digits) - 1:\n",
    "            divider = make_axes_locatable(ax[i,j])\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            fig.colorbar(im_confusion, cax=cax)\n",
    "            \n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.savefig('MLR_MNIST_test_ex2.pdf', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probit function \n",
    "from scipy.stats import norm\n",
    "\n",
    "def probit(x):\n",
    "    return norm.cdf(x) # Yes, it is exactly the standard normal CDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot probit and sigmoid function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[10,3])\n",
    "x = np.linspace(-7, 7, 100)\n",
    "ax.plot(x, sigmoid(x), color='blue', label=\"$y=\\sigma(x)=\\exp(x)/(1+\\exp(x))$\")\n",
    "ax.plot(x, probit(x), color='red', label=\"$y=\\psi(x)=Probit(x)$\")\n",
    "plt.axhline(y=1, color='g', linestyle='--')\n",
    "plt.axvline(x=0, color='g', linestyle='--')\n",
    "ax.legend()\n",
    "plt.savefig('probit_ex.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_PR_GD(Y, H, W0=None, sub_iter=100, stopping_diff=0.01):\n",
    "        '''\n",
    "        Convex optimization algorithm for Probit Regression using Gradient Descent \n",
    "        Y = (n x 1), H = (p x n) (\\Phi in lecture note), W = (p x 1)\n",
    "        Logistic Regression: Y ~ Bernoulli(Q), Q = Probit(H.T @ W)\n",
    "        '''\n",
    "        if W0 is None:\n",
    "            W0 = 1-2*np.random.rand(H.shape[0],1) #If initial coefficients W0 is None, randomly initialize from [-1,1] \n",
    "            \n",
    "        W1 = W0.copy()\n",
    "        i = 0\n",
    "        grad = np.ones(W0.shape)\n",
    "        while (i < sub_iter) and (np.linalg.norm(grad) > stopping_diff):\n",
    "            Q = norm.pdf(H.T @ W1) * ( (1-Y)/norm.cdf(-H.T @ W1) - Y/norm.cdf(H.T @ W1) )\n",
    "            grad = H @ Q\n",
    "            W1 = W1 - (np.log(i+1) / (((i + 1) ** (0.5)))) * grad\n",
    "            i = i + 1\n",
    "            # print('iter %i, grad_norm %f' %(i, np.linalg.norm(grad)))\n",
    "        return W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fitted probit regression curve\n",
    "digit_list_list = [['0','1'],['0','7'],['2','3'],['2', '8']] # list of list of two digits\n",
    "# fit LR for each cases\n",
    "W_array = []\n",
    "for i in np.arange(len(digit_list_list)):\n",
    "    L = digit_list_list[i]\n",
    "    X_train, X_test, y_train, y_test = sample_binary_MNIST(list_digits=L, full_MNIST = [X,y], noise_rate=0.5)\n",
    "    H_train = np.vstack((np.ones(X_train.shape[0]), X_train.T))  # add first row of 1's for bias features \n",
    "    \n",
    "    W = fit_PR_GD(Y=y_train, H=H_train/1000) \n",
    "    W = fit_PR_GD(Y=y_train, H=H_train/1000) \n",
    "    W_array.append(W.copy())\n",
    "\n",
    "W_array = np.asarray(W_array)\n",
    "\n",
    "\n",
    "# make plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(digit_list_list), figsize=[16, 4])\n",
    "for i in np.arange(len(digit_list_list)):\n",
    "    L = digit_list_list[i]\n",
    "    W = W_array[i]\n",
    "    im = ax[i].imshow(W[1:,:].reshape(28,28), vmin=np.min(W_array), vmax=np.max(W_array))\n",
    "    ax[i].title.set_text(\"LR coeff. for %s vs. %s\" % (L[0], L[1])) \n",
    "    # ax[i].legend()\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig('PR_MNIST_training_ex.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit probit regression using GD and compute binary classification accuracies\n",
    "\n",
    "# Get train and test data \n",
    "digits_list = ['4', '7']\n",
    "X_train, X_test, y_train, y_test = sample_binary_MNIST(digits_list, full_MNIST = [X,y], noise_rate=0.5)\n",
    "# Feature matrix of size (p x n) = (feature dim x samples)\n",
    "\n",
    "list_train_size = [1,10, 30, 100]\n",
    "\n",
    "# train the regression coefficients for all cases\n",
    "W_list = []\n",
    "results_list = []\n",
    "\n",
    "for i in np.arange(len(list_train_size)):\n",
    "    size = list_train_size[i]\n",
    "    idx = np.random.choice(np.arange(len(y_train)), size)\n",
    "    X_train0 = X_train[idx, :]\n",
    "    y_train0 = y_train[idx]\n",
    "\n",
    "    # Train the logistic regression model \n",
    "    H_train0 = np.vstack((np.ones(X_train0.shape[0]), X_train0.T))  # add first row of 1's for bias features \n",
    "    W = fit_PR_GD(Y=y_train0, H=H_train0/100)  # reduce the scale of H for numerical stability \n",
    "    W_list.append(W.copy()) # make sure use copied version of W since the same name is overrided in the loop \n",
    "\n",
    "    # Get predicted probabilities \n",
    "    H_test = np.vstack((np.ones(X_test.shape[0]), X_test.T))\n",
    "    Q = 1 / (1 + np.exp(-H_test.T @ W)) # predicted probabilities for y_test\n",
    "\n",
    "    # Compute binary classification accuracies\n",
    "    results_dict = compute_accuracy_metrics(Y_test=y_test, P_pred = Q)\n",
    "    results_dict.update({'train size':X_train0.shape[0]})  # add the train data size to the results dictionary\n",
    "    results_list.append(results_dict.copy())\n",
    "   \n",
    "    # Print out the results \n",
    "    \"\"\"\n",
    "    keys_list = [i for i in results_dict.keys()]\n",
    "    for key in keys_list:\n",
    "        if key not in ['Y_test', 'Y_pred']:\n",
    "            print('%s = %f' % (key, results_dict.get(key)))\n",
    "    \"\"\"\n",
    "\n",
    "# make plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(list_train_size), figsize=[16, 4])\n",
    "\n",
    "for i in np.arange(len(list_train_size)):\n",
    "    result_dict = results_list[i]\n",
    "    W = W_list[i][1:,:] \n",
    "    im = ax[i].imshow(W.copy().reshape(28,28), vmin=np.min(W_list), vmax=np.max(W_list))\n",
    "    \n",
    "    subtitle = \"\"\n",
    "    keys_list = [i for i in results_list[i].keys()]\n",
    "    for key in keys_list:\n",
    "        if key not in ['Y_test', 'Y_pred', 'AUC', 'Opt_threshold']:\n",
    "            subtitle += \"\\n\" + str(key) + \" = \" + str(np.round(results_list[i].get(key),3))\n",
    "            # print('%s = %f' % (key, results_list[i].get(key)))\n",
    "            \n",
    "    ax[i].set_title('Opt. regression coeff.', fontsize=13) \n",
    "    ax[i].set_xlabel(subtitle, fontsize=20) \n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "fig.suptitle(\"MNIST Binary Classification by LR for %s vs. %s\" % (digits_list[0], digits_list[1]), fontsize=20, y=1.05)\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig('PR_MNIST_test_ex1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_PTG_graphs(args, motif_size=1, subsample_ratio=1):\n",
    "\n",
    "    X,Y=prep_binary_classification_PTG(motif_size=motif_size, subsample_ratio=subsample_ratio)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "\n",
    "    X_train_list = []\n",
    "    for i in np.arange(X_train.shape[0]):\n",
    "        X_train_list.append(X_train[i,:])\n",
    "\n",
    "    print('y_train', y_train)\n",
    "\n",
    "    clf.fit(X_train_list, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "    print('y_test', y_test)\n",
    "    print('y_pred', y_pred)\n",
    "\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print('!!! subgraph classification accuracy using SVC', acc)\n",
    "    \"\"\"\n",
    "\n",
    "    ########### Logistic Regression ###########\n",
    "    X_train /= np.max(X_train)\n",
    "    y_train1 = list2onehot(y_train, list_classes=[0,1])\n",
    "    y_test1 = list2onehot(y_test, list_classes=[0,1])\n",
    "    print('y_test1', y_test1.shape)\n",
    "    print('y_train1', y_train1.shape)\n",
    "    print('X_train.T', X_train.T.shape)\n",
    "\n",
    "    H_train = np.vstack((np.ones(X_train.shape[0]), X_train.T))\n",
    "    W = fit_MLR_GD(y_train1, H_train, sub_iter=100, stopping_diff=0.01)\n",
    "    # Get predicted probabilities\n",
    "\n",
    "    print('W.shape', W.shape)\n",
    "    H_test = np.vstack((np.ones(X_test.shape[0]), X_test.T))\n",
    "    Q = softmax(H_test.T @ W.copy()) # predicted probabilities for y_test # Uses sklearn's softmax for numerical stability\n",
    "\n",
    "    print('Q.shape', Q.shape)\n",
    "    print('y_test1.shape', y_test1.shape)\n",
    "\n",
    "    results_dict = multiclass_accuracy_metrics(Y_test=y_test1, P_pred=Q)\n",
    "    confusion_mx = results_dict.get('results_dict')\n",
    "\n",
    "    ########### FFNN ########\n",
    "    X_train /= np.max(X_train)\n",
    "    y_train0 = list2onehot(y_train, list_classes=[0,1])\n",
    "    # preprocessing\n",
    "    out = []\n",
    "    # populate the tuple list with the data\n",
    "    for i in range(X_train.shape[0]):\n",
    "        item = list((X_train[i,:], y_train0[i,:]))\n",
    "        out.append(item)\n",
    "\n",
    "    # FFNN training\n",
    "    NN = DeepFFNN(hidden_layer_sizes=[100], training_data = out)\n",
    "    NN.train(iterations=200, learning_rate = 0.5, momentum = 0.1, rate_decay = 0.01, verbose=False)\n",
    "\n",
    "    # FFNN prediction\n",
    "    X_test /= np.max(X_test)\n",
    "    out_test = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        out_test.append(X_test[i,:].tolist())\n",
    "\n",
    "    y_hat = NN.predict(out_test).T\n",
    "\n",
    "    y_test_label = np.asarray(y_test)\n",
    "    P_pred = np.asarray([p[1] for p in y_hat])\n",
    "\n",
    "    print('!!y_test_label', y_test_label)\n",
    "    print('!!P_pred', P_pred)\n",
    "\n",
    "    compute_accuracy_metrics(Y_test=y_test_label, P_pred=P_pred, use_opt_threshold=False, verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colourgraphenv",
   "language": "python",
   "name": "colourgraphenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
